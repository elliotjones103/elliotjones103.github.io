{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no1y9tRBrzZ8"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-vbUZEStyb5"
      },
      "source": [
        "Run this cell to import the following libraries (they will be necessary for what follows)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiubMQh8qptD"
      },
      "outputs": [],
      "source": [
        "# Install cuda 1.21 if needed, so we can use our GPU\n",
        "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SZ_iYpsQobvF"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import tarfile\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import torch.nn.functional as fun\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNiN4iTJMcOU"
      },
      "source": [
        "# Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZj2Fe6uuJLS"
      },
      "source": [
        "In this assignment you will use a convolutional neural network to make predictions concerning images in the CIFAR10 dataset. CIFAR10 is a dataset of 60,000 RGB images where each channel is 32x32 pixels. Typically, 50,000 of these images are used for training and 10,000 are used for testing. It is strongly recommended you develop your code and this notebook on colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4zILHOXuN8x"
      },
      "source": [
        "### Some general points on notation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W52r3HYB4PQa"
      },
      "source": [
        "Typically we use\n",
        "- $[n] = \\{ 1,2,3...n\\}$, $[m,n] = \\{m, m+1... n\\}$ to denote sets frequently used for indices,\n",
        "- $\\mathbb{1}(E)$ is the indicator function for event E: for example, $\\mathbb{1}(a>b) = 1$ if $a>b$ and is $0$ otherwise.\n",
        "- $L$ as the scalar valued loss of the network on a batch,\n",
        "- $x$ as the tensor of input features,\n",
        "- $y$ as the tensor of labels,\n",
        "- $a$ as the output tensor from a layer,\n",
        "- $z$ as the input tensor to a layer.\n",
        "- $\\frac{\\partial L}{\\partial a}$ is a tensor with the same shape as $a$ but with each element being the partial derivative of $L$ with respect to the corresponding element in $a$. For example, if $a \\in\\mathbb{R}^{N,M,O}$ then $\\frac{\\partial L}{\\partial a} \\in \\mathbb{R}^{N,M,O}$ and $\\left[\\frac{\\partial L}{\\partial a} \\right]_{i,j,k} = \\frac{\\partial L}{\\partial a_{i,j,k}}$.\n",
        "- For a given layer with input tensor $z$ and output tensor $a$, we use $\\delta = \\frac{\\partial L}{\\partial a}$ to denote the error signal tensor propagated during the backward pass from the layer above, and $\\delta_{new} = \\frac{\\partial L}{\\partial z}$ to denote the error signal tensor propagated from the current layer to the one below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYZnID4dvCUB"
      },
      "source": [
        "### High level hints and tips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0ECClPKvHaB"
      },
      "source": [
        "1. Everytime you write a function or class you need to think of ways you can test it to make sure it is working as it should be.\n",
        " - Always check that the shape of the tensors outputted by a layer or function make sense / agree with what they should be.\n",
        " - Use small, simple trial cases to test your functions and class member functions (methods). These trial cases should be easy for you to check by hand in order to confirm your functions are behaving correctly.\n",
        " - When possible, consider checking that your function outputs are the same versus their equivalents in Pytorch! This is the only acceptable use of Pytorch during this assignment.\n",
        "\n",
        "2. For each layer you will need to implement a backward method to backpropagate error signals. You will in every case have to derive the backpropagation rule and try to implement it efficiently. Suppose $L$ is some scalar valued loss, $a$ is, e.g., a three dimensional tensor which is the output of the forward function of a layer, and $z$ is also a three dimensional tensor which is the input to the aforementioned layer. Then\n",
        " - first, compute $\\frac{\\partial a_{i,j,k}}{ \\partial z_{m,n,o}}$,\n",
        " - second, substitute this into the chain rule, $ \\frac{\\partial L}{\\partial z_{m,n,o}} = \\sum_{i,j,k}\\frac{\\partial L}{\\partial a_{i,j,k}} \\frac{ \\partial a_{i,j,k}}{\\partial z_{m,n,o}}$.\n",
        " - Third, look for patterns that allow you to represent $\\frac{\\partial L}{\\partial z}$ succinctly.\n",
        "\n",
        "3. Although you are awarded some points for the performance of your model, many of the marks in Part II come from demonstrating and communicating a clear approach to and awareness and understanding of how to improve the training and generalization of neural networks.\n",
        "\n",
        "4. This coursework is designed to be challenging, you should not expect to answer every question perfectly. Don't stress if you struggle at times and also think carefully about diminishing returns: you have limited time given your other academic commitments, hard questions are worth fewer marks and also will likely take more time to solve!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ90qS_UbaGH"
      },
      "source": [
        "# PART I\n",
        "In this part of the coursework we build on the final lab and use Pytorch to develop a performant model for processing the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwFCw4X-cJr5"
      },
      "source": [
        "# Q1. [25 marks] Build and train a CNN in Pytorch to process CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_tUEQZLcakZ"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWXSMeAOcSh3"
      },
      "source": [
        "Building on the final lab, use Pytorch to define and train a model which achieves a high prediction accuracy on test data (not used during training to select parameters or hyperparameters). Feel free to use the code from your last lab to get you started. This question is worth 40 marks and you will receive marks for\n",
        " - your final model's performance / test score (test accuracy),\n",
        " - evidence you have attempted to improve performance using at least some combination of the techniques outlined below (or others). In short, I expect you to build on and adapt the code provided in lab 9.\n",
        "\n",
        "Things you might want to try to improve training and test accuracy.\n",
        "- Try different architectures and layers, see what happens if you include or don't include certain layers, identify computational bottlenecks and take steps to mitigate overfitting.\n",
        "- When you import the data think about preprocessing techniques: for example, normalizing the data so that pixel values in each channel have mean zero and standard deviation one. If your model is not generalizing well then try data augmentation techniques such as cropping, blurring or rotation.\n",
        "- Consider using k-fold cross validation to find good hyperparameters.\n",
        "- Instead of using SGD experiment with other optimizers: for example, momentum, RMSprop or ADAM, compare the resulting error curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpBd4Mmvdk3J"
      },
      "source": [
        "### Develop and train your model to perform well on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EvMzTFOqptF"
      },
      "source": [
        "# Load data and CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VjXxT5dX7Ydh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 512\n",
        "\n",
        "# Normalization parameters for CIFAR-10 (standard)\n",
        "# These are the per-channel means and stds computed over the CIFAR-10 dataset\n",
        "mean = [0.4914, 0.4822, 0.4465]\n",
        "std = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "# Transformations for the training data\n",
        "transform_train = transforms.Compose([\n",
        " #   transforms.RandomCrop(32, padding=4),\n",
        " #   transforms.RandomHorizontalFlip(),\n",
        " #   transforms.RandomRotation(15),\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std),\n",
        "#    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2))\n",
        "])\n",
        "\n",
        "# Transformations for the testing data\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Load training and validation datasets\n",
        "trainset = datasets.CIFAR10(root='./CIFAR10_data/', train=True, download=True, transform=transform_train)\n",
        "#trainset_partial = Subset(trainset_full, list(range(3000)))\n",
        "valset = datasets.CIFAR10(root='./CIFAR10_data/', train=False, download=True, transform=transform_test)\n",
        "#valset_partial = Subset(valset_full, list(range(3000)))\n",
        "\n",
        "# Prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Class names\n",
        "class_names = trainset.classes\n",
        "\n",
        "# Define CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Five convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding = 1, bias=False)\n",
        "\n",
        "        # Apply batchnorm\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
        "        self.batchnorm4 = nn.BatchNorm2d(512)\n",
        "        self.batchnorm5 = nn.BatchNorm2d(1024)\n",
        "\n",
        "        # Define the ReLU\n",
        "        self.ReLU = nn.ReLU()\n",
        "\n",
        "        # Define the maxpooling layer\n",
        "        self.maxpooling = nn.MaxPool2d(2)\n",
        "\n",
        "        # Define the dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        # Define global adaptive average pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # outputs (B,512,1,1)\n",
        "\n",
        "        # Define fully connected layer\n",
        "        self.fc  = nn.Linear(1024, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Apply Conv followed by ReLU, then max pooling and drop out for each layer.\n",
        "        x = self.ReLU(self.conv1(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm1(x))\n",
        "\n",
        "        x = self.ReLU(self.conv2(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm2(x))\n",
        "\n",
        "        x = self.ReLU(self.conv3(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm3(x))\n",
        "\n",
        "        x = self.ReLU(self.conv4(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm4(x))\n",
        "\n",
        "        x = self.ReLU(self.conv5(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm5(x))\n",
        "\n",
        "        # Apply global adaptave average pooling\n",
        "        x = self.gap(x)              # (B,512,1,1)\n",
        "\n",
        "        # Reshape for fully connected layer\n",
        "        x = x.view(x.size(0), 1024)   # (B,512)\n",
        "\n",
        "        # Apply fully connected layer\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "# Initialize the NN\n",
        "model = Net()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Lists to store running losses and accuracies of both datasets\n",
        "train_losses_list, val_losses_list = [], []\n",
        "train_acc_list, val_acc_list = [], []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw40yso4qptG"
      },
      "source": [
        "# K-Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "4nejsdJWqptG",
        "outputId": "8336f438-ed36-492c-c09b-341061043335"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grid lr=1e-03, wd=0e+00 → Fold 1/3\n",
            "  Epoch 1/8 → Train: 1.935/0.333, Val: 1.629/0.413\n",
            "  Epoch 2/8 → Train: 1.431/0.482, Val: 1.509/0.489\n",
            "  Epoch 3/8 → Train: 1.222/0.565, Val: 1.222/0.575\n",
            "  Epoch 4/8 → Train: 1.070/0.621, Val: 1.164/0.601\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25968\\3521772917.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[0mval_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m                         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                         \u001b[0mmodel_pred\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             if (\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1448\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1400\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1402\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1403\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1241\u001b[0m         \u001b[1;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1243\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1244\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "batch_size = 512\n",
        "n_epochs   = 8\n",
        "k_folds = 3\n",
        "\n",
        "# Hyperparameter grid\n",
        "grid = {\n",
        "    \"lr\":           [0.001, 0.002, 0.0015, 0.003],\n",
        "    \"weight_decay\": [0, 0.001, 0.002, 0.003, 0.005, 0.01]\n",
        "    }\n",
        "\n",
        "# Prepare k-fold splitter\n",
        "kf = KFold(n_splits=k_folds, shuffle=True)\n",
        "best_overall = {\"config\": None, \"score\": 0.0}\n",
        "\n",
        "# Specify loss function\n",
        "cross_ent_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "for lr in grid[\"lr\"]:\n",
        "    for wd in grid[\"weight_decay\"]:\n",
        "        fold_scores = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(trainset), 1):\n",
        "            print(f\"Grid lr={lr:.0e}, wd={wd:.0e} → Fold {fold}/{k_folds}\")\n",
        "\n",
        "            # Load model and optimiser\n",
        "            model     = Net().to(device)\n",
        "            optimiser = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "            # Load data\n",
        "            train_loader = torch.utils.data.DataLoader(Subset(trainset, train_idx), batch_size=batch_size, shuffle=True,  num_workers=1, pin_memory=True)\n",
        "            val_loader   = torch.utils.data.DataLoader(Subset(trainset, val_idx), batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
        "            best_fold_acc = 0.0\n",
        "\n",
        "            # Traing the model\n",
        "            for epoch in range(1, n_epochs+1):\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                running_correct = 0\n",
        "                for X, y in train_loader:\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    optimiser.zero_grad()\n",
        "                    model_pred = model(X)\n",
        "                    loss  = cross_ent_loss(model_pred, y)\n",
        "                    loss.backward()\n",
        "                    optimiser.step()\n",
        "\n",
        "                    running_loss    += loss.item() * X.size(0)\n",
        "                    running_correct += (model_pred.argmax(1) == y).sum().item()\n",
        "\n",
        "                # Record training loss and accuracy\n",
        "                train_loss = running_loss / len(train_idx)\n",
        "                train_acc  = running_correct / len(train_idx)\n",
        "\n",
        "                # Validation of the model\n",
        "                model.eval()\n",
        "                val_loss = 0.0\n",
        "                val_correct = 0\n",
        "                with torch.no_grad():\n",
        "                    for X, y in val_loader:\n",
        "                        X, y     = X.to(device), y.to(device)\n",
        "                        model_pred   = model(X)\n",
        "                        loss_val = cross_ent_loss(model_pred, y)\n",
        "                        val_loss   += loss_val.item() * X.size(0)\n",
        "                        val_correct+= (model_pred.argmax(1) == y).sum().item()\n",
        "\n",
        "                # Record validation loss and accuracy\n",
        "                val_loss /= len(val_idx)\n",
        "                val_acc   = val_correct / len(val_idx)\n",
        "\n",
        "                # Track the best values for fold\n",
        "                if val_acc > best_fold_acc:\n",
        "                    best_fold_acc = val_acc\n",
        "\n",
        "                # Printouts\n",
        "                print(f\"  Epoch {epoch}/{n_epochs} → \"\n",
        "                      f\"Train: {train_loss:.3f}/{train_acc:.3f}, \"\n",
        "                      f\"Val: {val_loss:.3f}/{val_acc:.3f}\")\n",
        "\n",
        "            fold_scores.append(best_fold_acc)\n",
        "            print(f\" → Fold {fold} best val‐acc: {best_fold_acc:.3f}\\n\")\n",
        "\n",
        "print(f\"*** Best hyperparams: lr={best_overall['config']['lr']:.0e}, \"\n",
        "      f\"wd={best_overall['config']['weight_decay']:.0e} \"\n",
        "      f\"→ CV acc={best_overall['score']:.3f} ***\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx8JmQwUqptG"
      },
      "source": [
        "# Reload data and CNN model for next test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BVXH_VraqptH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 512\n",
        "\n",
        "# Normalization parameters for CIFAR-10 (standard)\n",
        "# These are the per-channel means and stds computed over the CIFAR-10 dataset\n",
        "mean = [0.4914, 0.4822, 0.4465]\n",
        "std = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "# Transformations for the training data\n",
        "transform_train = transforms.Compose([\n",
        " #   transforms.RandomCrop(32, padding=4),\n",
        " #   transforms.RandomHorizontalFlip(),\n",
        " #   transforms.RandomRotation(15),\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std),\n",
        "#    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2))\n",
        "])\n",
        "\n",
        "# Transformations for the testing data\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Load training and validation datasets\n",
        "trainset_full = datasets.CIFAR10(root='./CIFAR10_data/', train=True, download=True, transform=transform_train)\n",
        "#trainset_partial = Subset(trainset_full, list(range(3000)))\n",
        "valset_full = datasets.CIFAR10(root='./CIFAR10_data/', train=False, download=True, transform=transform_test)\n",
        "#valset_partial = Subset(valset_full, list(range(3000)))\n",
        "\n",
        "# Prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(trainset_full, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(valset_full, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Class names\n",
        "class_names = trainset_full.classes\n",
        "\n",
        "# Define CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Five convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding = 1, bias=False)\n",
        "\n",
        "        # Apply batchnorm\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
        "        self.batchnorm4 = nn.BatchNorm2d(512)\n",
        "        self.batchnorm5 = nn.BatchNorm2d(1024)\n",
        "\n",
        "        # Define the ReLU\n",
        "        self.ReLU = nn.ReLU()\n",
        "\n",
        "        # Define the maxpooling layer\n",
        "        self.maxpooling = nn.MaxPool2d(2)\n",
        "\n",
        "        # Define the dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        # Define global adaptive average pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # outputs (B,512,1,1)\n",
        "\n",
        "        # Define fully connected layer\n",
        "        self.fc  = nn.Linear(1024, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Apply Conv followed by ReLU, then max pooling and drop out for each layer.\n",
        "        x = self.ReLU(self.conv1(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm1(x))\n",
        "\n",
        "        x = self.ReLU(self.conv2(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm2(x))\n",
        "\n",
        "        x = self.ReLU(self.conv3(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm3(x))\n",
        "\n",
        "        x = self.ReLU(self.conv4(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm4(x))\n",
        "\n",
        "        x = self.ReLU(self.conv5(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm5(x))\n",
        "\n",
        "        # Apply global adaptave average pooling\n",
        "        x = self.gap(x)              # (B,512,1,1)\n",
        "\n",
        "        # Reshape for fully connected layer\n",
        "        x = x.view(x.size(0), 1024)   # (B,512)\n",
        "\n",
        "        # Apply fully connected layer\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "# Initialize the NN\n",
        "model = Net()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Lists to store running losses and accuracies of both datasets\n",
        "train_losses_list, val_losses_list = [], []\n",
        "train_acc_list, val_acc_list = [], []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAmYIsfXqptH"
      },
      "source": [
        "# Our Final CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "2hFE4dXlqptH",
        "outputId": "adedc7be-925d-4cad-bec8-bf8bbeb90dd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/10 (Time: 40.64s), Training Loss: 1.836, Training Accuracy: 0.368, Validation Loss: 1.198, Validation Accuracy: 0.562\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19704\\3950127056.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Train our CNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmodel_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             if (\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\datasets\\cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define variable to track the highest running accuracy\n",
        "top_val_acc = 0\n",
        "count = 0\n",
        "\n",
        "# Define loss and optimiser\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Iterate over the number of epochs\n",
        "n_epochs = 10\n",
        "model.train()\n",
        "for n in range(n_epochs):\n",
        "    epoch_start = time.time()\n",
        "    tot_train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    # Train our CNN\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        model_pred = model(X)\n",
        "        loss_value = loss(model_pred, y)\n",
        "        tot_train_loss += loss_value.item() * X.size(0)\n",
        "        loss_value.backward()\n",
        "        optimiser.step()\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        predicted_class = torch.argmax(model_pred, dim=1)\n",
        "        train_correct += torch.sum((predicted_class == y).float())\n",
        "\n",
        "    train_loss = tot_train_loss / len(train_loader.dataset)\n",
        "    train_accuracy = train_correct / len(train_loader.dataset)\n",
        "\n",
        "    # Validate our CNN\n",
        "    tot_val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            model_pred = model(X)\n",
        "            loss_value = loss(model_pred, y)\n",
        "            tot_val_loss += loss_value.item() * X.size(0)\n",
        "            predicted_class = torch.argmax(model_pred, dim=1)\n",
        "            val_correct += torch.sum((predicted_class == y).float())\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    val_loss = tot_val_loss / len(val_loader.dataset)\n",
        "    val_accuracy = val_correct / len(val_loader.dataset)\n",
        "\n",
        "    # Track time it takes to complete each epoch\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses_list.append(train_loss)\n",
        "    val_losses_list.append(val_loss)\n",
        "    train_acc_list.append(train_accuracy)\n",
        "    val_acc_list.append(val_accuracy)\n",
        "\n",
        "    # Checkpoint the best model\n",
        "    if val_accuracy > top_val_acc:\n",
        "        torch.save(model, 'top_cnn_model')\n",
        "        top_val_acc = val_accuracy\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "\n",
        "    # Printouts and plotting\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Epoch: {n+1}/{n_epochs} (Time: {epoch_time:.2f}s), \"\n",
        "          f\"Training Loss: {train_loss:.3f}, Training Accuracy: {train_accuracy:.3f}, \"\n",
        "          f\"Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.3f}\")\n",
        "    plt.figure(figsize=(15,4))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(train_losses_list, label='Train loss')\n",
        "    plt.plot(val_losses_list, label='Validation loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(torch.tensor(train_acc_list).cpu(), label='Train accuracy')\n",
        "    plt.plot(torch.tensor(val_acc_list).cpu(), label='Validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"Training complete. Best validation accuracy:\", top_val_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGbveF1cqptH"
      },
      "source": [
        "# Reload data and CNN model for next test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EEYiFT3sqptH",
        "outputId": "a0974b85-a6bd-401d-bf2f-97e1622afc70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 512\n",
        "\n",
        "# Normalization parameters for CIFAR-10 (standard)\n",
        "# These are the per-channel means and stds computed over the CIFAR-10 dataset\n",
        "mean = [0.4914, 0.4822, 0.4465]\n",
        "std = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "# Transformations for the training data\n",
        "transform_train = transforms.Compose([\n",
        " #   transforms.RandomCrop(32, padding=4),\n",
        " #   transforms.RandomHorizontalFlip(),\n",
        " #   transforms.RandomRotation(15),\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std),\n",
        "#    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2))\n",
        "])\n",
        "\n",
        "# Transformations for the testing data\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Load training and validation datasets\n",
        "trainset_full = datasets.CIFAR10(root='./CIFAR10_data/', train=True, download=True, transform=transform_train)\n",
        "#trainset_partial = Subset(trainset_full, list(range(3000)))\n",
        "valset_full = datasets.CIFAR10(root='./CIFAR10_data/', train=False, download=True, transform=transform_test)\n",
        "#valset_partial = Subset(valset_full, list(range(3000)))\n",
        "\n",
        "# Prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(trainset_full, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(valset_full, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Class names\n",
        "class_names = trainset_full.classes\n",
        "\n",
        "# Define CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Five convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding = 1, bias=False)\n",
        "        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding = 1, bias=False)\n",
        "\n",
        "        # Apply batchnorm\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
        "        self.batchnorm4 = nn.BatchNorm2d(512)\n",
        "        self.batchnorm5 = nn.BatchNorm2d(1024)\n",
        "\n",
        "        # Define the ReLU\n",
        "        self.ReLU = nn.ReLU()\n",
        "\n",
        "        # Define the maxpooling layer\n",
        "        self.maxpooling = nn.MaxPool2d(2)\n",
        "\n",
        "        # Define the dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        # Define global adaptive average pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # outputs (B,512,1,1)\n",
        "\n",
        "        # Define fully connected layer\n",
        "        self.fc  = nn.Linear(1024, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Apply Conv followed by ReLU, then max pooling and drop out for each layer.\n",
        "        x = self.ReLU(self.conv1(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm1(x))\n",
        "\n",
        "        x = self.ReLU(self.conv2(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm2(x))\n",
        "\n",
        "        x = self.ReLU(self.conv3(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm3(x))\n",
        "\n",
        "        x = self.ReLU(self.conv4(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm4(x))\n",
        "\n",
        "        x = self.ReLU(self.conv5(x))\n",
        "        x = self.maxpooling(x)\n",
        "        x = self.dropout(self.batchnorm5(x))\n",
        "\n",
        "        # Apply global adaptave average pooling\n",
        "        x = self.gap(x)              # (B,512,1,1)\n",
        "\n",
        "        # Reshape for fully connected layer\n",
        "        x = x.view(x.size(0), 1024)   # (B,512)\n",
        "\n",
        "        # Apply fully connected layer\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "# Initialize the NN\n",
        "model = Net()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Lists to store running losses and accuracies of both datasets\n",
        "train_losses_list, val_losses_list = [], []\n",
        "train_acc_list, val_acc_list = [], []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4Bc96bwqptI"
      },
      "source": [
        "# Testing our Final CNN with a learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HHIo2ejKqptI",
        "outputId": "ae2c3036-dc49-4a62-df05-b11987c92ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/30 (Time: 40.85s), Training Loss: 1.685, Training Accuracy: 0.406, Validation Loss: 1.121, Validation Accuracy: 0.601\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19704\\3890772147.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Train our CNN model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             if (\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\datasets\\cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \"\"\"\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\transforms\\_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    920\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m     \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 922\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    923\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"std evaluated to zero after conversion to {dtype}, leading to division by zero.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Variable to track the highest running accuracy\n",
        "top_val_acc = 0\n",
        "count = 0\n",
        "\n",
        "# Define loss, optimiser, and LR scheduler\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimiser, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "n_epochs = 30\n",
        "model.train()\n",
        "\n",
        "# Iterate over the number of epochs\n",
        "for n in range(n_epochs):\n",
        "    epoch_start = time.time()\n",
        "    tot_train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    # Train our CNN model\n",
        "    for X, y in train_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        model_pred = model(X)\n",
        "        loss_value = loss(model_pred, y)\n",
        "        tot_train_loss += loss_value.item() * X.size(0)\n",
        "\n",
        "        loss_value.backward()\n",
        "        optimiser.step()\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        predicted_class = torch.argmax(model_pred, dim=1)\n",
        "        train_correct += torch.sum((predicted_class == y).float())\n",
        "\n",
        "    train_loss = tot_train_loss / len(train_loader.dataset)\n",
        "    train_accuracy = train_correct / len(train_loader.dataset)\n",
        "\n",
        "    # Validate our CNN model\n",
        "    tot_val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            model_pred = model(X)\n",
        "            loss_value = loss(model_pred, y)\n",
        "            tot_val_loss += loss_value.item() * X.size(0)\n",
        "            predicted_class = torch.argmax(model_pred, dim=1)\n",
        "            val_correct += torch.sum((predicted_class == y).float())\n",
        "    model.train()\n",
        "\n",
        "    val_loss = tot_val_loss / len(val_loader.dataset)\n",
        "    val_accuracy = val_correct / len(val_loader.dataset)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Record time it takes to complete each epoch\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses_list.append(train_loss)\n",
        "    val_losses_list.append(val_loss)\n",
        "    train_acc_list.append(train_accuracy)\n",
        "    val_acc_list.append(val_accuracy)\n",
        "\n",
        "    # Checkpoint the best model\n",
        "    if val_accuracy > top_val_acc:\n",
        "        torch.save(model, 'top_cnn_model')\n",
        "        top_val_acc = val_accuracy\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "\n",
        "    # Printouts and plotting\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Epoch: {n+1}/{n_epochs} (Time: {epoch_time:.2f}s), \"\n",
        "          f\"Training Loss: {train_loss:.3f}, Training Accuracy: {train_accuracy:.3f}, \"\n",
        "          f\"Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.3f}\")\n",
        "    plt.figure(figsize=(15,4))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(train_losses_list, label='Train loss')\n",
        "    plt.plot(val_losses_list, label='Validation loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(torch.tensor(train_acc_list).cpu(), label='Train accuracy')\n",
        "    plt.plot(torch.tensor(val_acc_list).cpu(), label='Validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"Training complete. Best validation accuracy:\", top_val_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTvwX6NM0b5e"
      },
      "source": [
        "# Q2. [15 marks] Report analyzing and discussing your CNN model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-uIpFiX2LJr"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95bO9c4l18Yc"
      },
      "source": [
        "Once you are content with the perfomance of your model write a concise report (equivalent of max 2 pages A4 without figures) below detailing the following.\n",
        "- Walk me through the different development stages of your final model, including the different things you tried to improve training and or generalization.\n",
        "- Discuss your final model and its train versus test accuracy, Make sure to provide plots of the training and test loss throughout training and comment on them, e.g., do you observe any underfitting or overfitting?\n",
        "- Discuss where you think the key bottlenecks of your model are in terms of compute and generalization and what you would do moving forward to improve performance.\n",
        "\n",
        "Note: you should support your assertions and claims with experimental evidence. This question is worth 15 marks, you will recieve marks for\n",
        " - your analysis of the performance of your model,\n",
        " - the clarity and logic of your report: in particular, what steps did you pursue to improve model performance and why?\n",
        " - The accuracy of your reflections and conclusions.\n",
        " - The quality of the empirical evidence, particularly in the form of graphs and plots, that you provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ28BNhf2Nzr"
      },
      "source": [
        "### Your report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjhMWXiUuyl-"
      },
      "source": [
        "# PART II\n",
        "In this part of the coursework we return to basics and develop the key classes (layers) and functions used to build and train a CNN from scratch, based on NumPy. You can use Pytorch to test your code, e.g., by comparing the outputs of your layer with the equivalent in Pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tasZhRkTmBQO"
      },
      "source": [
        "# Q3. [10 marks] Combining softmax with cross entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlJWOUONWYRJ"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBDV3CO9E9Ra"
      },
      "source": [
        "Our goal is to train a model which outputs class predictions given a feature vector. One way we can achieve this is by designing our model so that it outputs a probability mass function over the possible classes in our dataset. This can be achieved using the softmax function.\n",
        "\n",
        "\n",
        "#### Softmax function:\n",
        "Let $c$ be the number of different class labels in our data. Recall, given an input vector $z \\in \\mathbb{R}^c$, that the $i$th output (for $i \\in [c]$) of the softmax function, which we denote as $\\sigma: \\mathbb{R}^c \\rightarrow \\mathbb{R}^c$, is\n",
        "$$\\sigma_i(x) = \\frac{\\exp(z_i)}{ \\sum_{j=1}^c \\exp(z_j)}.$$\n",
        "Note for any input $z$ we have $\\sigma_i(z)\\geq 0$, moreover, by construction $\\sum_{i \\in [c]} \\sigma_i(z) = 1$. Therefore $\\sigma(z)$ defines a probability mass function where we can interpret $\\sigma_i(z)$ as the probability assigned to the event that the label of $z$ is $i \\in [c]$. Finally, it is common to compute softmax as follows: if $z^* = max_{i \\in [c]}z_i$ then\n",
        "$$\n",
        " \\sigma_i(z) = \\frac{\\exp(z_i- z^*)}{ \\sum_{j=1}^c \\exp(z_j - z^*)}.\n",
        "$$\n",
        "This helps with the numerical stability of computing $e^x$ for large x by ensuring x is at most $0$.\n",
        "\n",
        "\n",
        "#### The cross entropy loss:\n",
        "In order to measure the performance of our model we need to define a loss function. For multiclass classification a popular and natural choice is to use the cross entropy loss, which we now motivate and define. Consider probability distributions (or mass functions) over $c$ objects (we denote these objects using the set $[c]$). To this end let $p, q$ be two mass functions on $[c]$, which we can equivalently think of as vectors $p,q \\in [0,1]^c$ where the $i$th element is the probability assigned to $i \\in [c]$. The *cross entropy* between $p$ and $q$ is defined as\n",
        "$$\n",
        "H(p, q) = - \\sum_{i \\in [c]} p_i \\log(q_i),\n",
        "$$\n",
        "note this is non-negative and is non-symmetric. A useful identity for understanding the cross entropy is the following,\n",
        "$$\n",
        "H(p,q) = H(p,p) + D_{KL}(p || q),\n",
        "$$\n",
        "where here $D_{KL}$ is a scalar valued function (called the Kullback Leibler function) which measures the dissimilarity between $p$ and $q$. $D_{KL}$ is non-negative and moreover is zero if and only if $p=q$. We won't go into this in any more detail, suffice to say this identity implies that the cross entropy is minimized when $p = q$.\n",
        "\n",
        "We can use cross entropy to define a loss function for our model. To this end, for $p$ we use the one hot encoding of a label $y \\in [c]$, which we denote $\\tilde{y} \\in \\{0,1\\}^c$: this is a probability mass function over the class labels with all the mass concentrated on just one class (the true class of the input). Using softmax as our output layer, we can ensure our network outputs a vector $\\hat{y} \\in [0,1]^c$ that encodes a probability mass function over the labels and substitute this in for $q$. Therefore the accuracy of our prediction for a given example can be measured as\n",
        "$$\n",
        "\\begin{align*}\n",
        "H(\\tilde{y}, \\hat{y}) = - \\sum_{i \\in [c]} \\tilde{y_i} \\log(\\hat{y_i}) = - \\log(\\hat{y_y}).\n",
        "\\end{align*}\n",
        "$$\n",
        "As a result, we define the *cross entropy loss* as $C_E(y, \\hat{y}):= -\\log(\\hat{y}_{y}) $. Note that if $\\hat{y_y}$ is close to 1 then the loss is close to 0, whereas if $\\hat{y_y}$ approaches $0$ then the loss diverges. Note: to avoid numerical issues it is sensible to clip $\\hat{y_y} \\geq \\epsilon > 0$. The total loss is then the average cross entropy loss over the batch.\n",
        "\n",
        "\n",
        "\n",
        "#### Objective\n",
        "Our goal is to implement a combined softmax cross entropy (SCE) class with the following member functions.\n",
        "- A softmax member function which computes the softmax function on an input tensor.\n",
        "- A cross_entropy function which calculates the average cross entropy between the predicted labels and the true labels.\n",
        "- A backward member function which returns the error signal for the previous layer $\\delta_{new}$ given the error signal $\\delta$ from the layer above. This member function should also store down as member variables the gradient with respect to the biases and weights.\n",
        "- A backward function which computes the gradient or derivative of the average cross entropy with respect to the input of the SCE layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTwTMHm2Whik"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7OFA0CHmKO3"
      },
      "source": [
        "<ol type = \"A\">\n",
        "  <li> Using the definition above and the comments in the code, complete the definition of the softmax method for the SCE class outlined below. For numerical stability, make sure to shift down by the largest element before applying the exponential function!</li>\n",
        "  <li> Using the definition above and the comments in the code, complete the definition of the cross_entropy method for the SCE class outlined below. We can't apply log to zero, so as discussed threshold the predictions from below away from zero.  </li>\n",
        "  <li> As discussed in the overview, let $\\tilde{y}$ denote the one hot encoding of a true label $y \\in [c]$, i.e., $\\tilde{y}_j = 1$ if $y = j$ and is $0$ otherwise. In the cell titled \"Derivative of the softmax cross entropy layer\", show that $\\frac{\\partial}{\\partial z} C_E(y, \\sigma(z)) = \\sigma(z) - \\tilde{y}$.</li>\n",
        "  <li> Complete the definition of the backward member function for the SCE class outlined below. You should vectorize your code by taking advantage of broadcasting and avoid for loops!</li>\n",
        "  <li> Demonstrate that your code works by designing some tests: in particular, in the cells below the heading \"Test your code\" describe what your tests are and demonstrate that your code passes your tests.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGyx_ZW-WnWc"
      },
      "source": [
        "### C. Derivative of the softmax cross entropy layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWXn7eDMw0vi"
      },
      "source": [
        "We want to show that\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial z} C_E(y, \\sigma(z)) = \\sigma(z) - \\tilde{y}\n",
        "\\end{align}\n",
        "Since we already know that $C_E(y, \\hat{y}):= -\\log(\\hat{y}_{y})$ we can notice that $ C_E(y, \\sigma(z)) = log(\\sigma_y(z))$. As this is a composite function we use the chain rule to derive the softmax cross entropy layer. We denote $s = \\sigma(z)$ as the vector of softmax outputs and $L(s) = -log(s_y)$, where y is the true label, to allow for easier reading and computation. we want to now compute,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial s} \\cdot \\frac{\\partial s}{\\partial z}\n",
        "\\end{align}\n",
        "for this we need, the gradient of the loss $\\frac{\\partial L}{\\partial s}$ and the jacobian of the softmax function $\\frac{\\partial s}{\\partial z}$.\n",
        "\n",
        "We first derive the loss with respect to the Softmax output, the loss is defined as,\n",
        "\\begin{align}\n",
        "L(s) = -log(s_y)\n",
        "\\end{align}\n",
        "This is a function of the softmax vector s, but we are only interested in the y-th component so we have,\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial s_i} = \\\n",
        "\\begin{cases}\n",
        "-\\frac{1}{s_i}, \\quad \\text{if} \\quad i = y \\\\\n",
        "0, \\qquad \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "This produces a vector that is zero everywhere except at y, where it is $\\frac{-1}{s_y}$. \\\n",
        "The softmax function is defined as\n",
        "\n",
        "\\begin{align}\n",
        "\\sigma_i(x) = \\frac{exp(z_i)}{\\sum_{j=1}^c exp(z_j)}\n",
        "\\end{align}\n",
        "\n",
        "We need to compute the jacobian matrix of the softmax function $J \\in \\mathbb{R} ^{c \\times c}$ where,\n",
        "\\begin{align}\n",
        "J_{i,j} = \\frac{\\partial s_i}{\\partial z_j}\n",
        "\\end{align}\n",
        "\n",
        "This results in:\n",
        "\\begin{align}\n",
        "\\frac{\\partial s_i}{\\partial z_j} =\n",
        "\\begin{cases}\n",
        "s_i(1-s_i), \\quad \\text{if} \\quad i=j \\\\\n",
        "-s_i s_j, \\quad \\text{if} \\quad  i \\neq  j\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "Using the chain rule we can now solve,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_j} = \\sum_{i=1}^c \\frac{\\partial L}{\\partial s_i} \\cdot \\frac{\\partial s_i}{\\partial z_j}\n",
        "\\end{align}\n",
        "\n",
        "Since we are only interested in the cases where $i=y$ we have,\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_j} = \\frac{\\partial L}{\\partial s_y} \\cdot \\frac{\\partial s_y}{\\partial z_j} =  -\\frac{1}{s_y} \\cdot  \\frac{\\partial s_y}{\\partial z_j}\n",
        "\\end{align}\n",
        "\n",
        "now for $\\frac{\\partial s_y}{\\partial z_j}$ we have two cases,\n",
        "\n",
        "- if $j=y$,\n",
        "\\begin{align}\n",
        "\\frac{\\partial s_y}{\\partial z_j} = s_y(1-s_y)\n",
        "\\end{align}\n",
        "therefore,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_y} = -\\frac{1}{s_y} \\cdot s_y(1-s_y) = s_y -1\n",
        "\\end{align}\n",
        "- if $j \\neq y$,\n",
        "\\begin{align}\n",
        "\\frac{\\partial s_y}{\\partial z_j} = -s_ys_j\n",
        "\\end{align}\n",
        "therefore,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_y} = -\\frac{1}{s_y} \\cdot (-s_ys_j) = s_j\n",
        "\\end{align}\n",
        "\n",
        "Finally, since $\\tilde{y}$ is denoted as the one hot encoding of a true label $y \\in [c]$, we can see that when $i=j$ $\\tilde{y}_i = 1$ and 0 when $i \\neq j$. Subbing back in $s = \\sigma(z)$ we see that we have exactly,\n",
        "\\begin{align}\n",
        "\\sigma(z) - \\tilde{y}\n",
        "\\end{align}\n",
        "\n",
        "as such we have shown that\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial z} C_E(y, \\sigma(z)) = \\sigma(z) - \\tilde{y}\n",
        "\\end{align}\n",
        "as required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uksaa48sW2QD"
      },
      "source": [
        "### A class for the softmax cross entropy layer (SCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4_oMGYxbmSw-"
      },
      "outputs": [],
      "source": [
        "class SCE:\n",
        "  \"\"\"\n",
        "  Class implementing a softmax layer feeding into the cross entropy loss function.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.eps = 1e-10 # Used to clip softmax inputs and avoid numerical instability\n",
        "    self.z = None # Stores input tensor to the softmax layer during forward pass (needed for the backward pass)\n",
        "    # Has no model parameters\n",
        "\n",
        "  def softmax(self, z):\n",
        "    # A\n",
        "    \"\"\"\n",
        "    Computes the softmax\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    z : numpy array of shape (batch_size, num_classes)\n",
        "        The input to the softmax function.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    y_pred: a numpy array of shape (batch_size, num_classes)\n",
        "        Softmax applied to the each row of z\n",
        "    \"\"\"\n",
        "    self.z = z # Store for use\n",
        "    z_shifted = z - np.max(z, axis = 1, keepdims = True) # Shifted for numerical stability\n",
        "    exp_z = np.exp(z_shifted)\n",
        "    y_pred = exp_z / np.sum(exp_z, axis = 1, keepdims = True)\n",
        "#    print(y_pred.shape) # Check shape of y_pred matches with (batch_size, num_classes) for testing and verification purposes\n",
        "    return y_pred\n",
        "\n",
        "  def cross_entropy(self, y_true, y_pred):\n",
        "    # B\n",
        "    \"\"\"\n",
        "    Computes the average cross-entropy loss for one-hot encoded predictions given scalar\n",
        "    target labels.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_true : numpy array, type int, shape = (batch_size,)\n",
        "          True labels or targets.\n",
        "    y_pred : numpy array, type float, shape = (batch_size, num_classes)\n",
        "          Predicted distribution over classes for each example.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    av_loss : float\n",
        "        The average cross-entropy loss across the batch.\n",
        "    \"\"\"\n",
        "    y_pred = np.clip(y_pred, self.eps, 1.0)\n",
        "    log_prob = -np.log(y_pred[np.arange(len(y_true)), y_true])\n",
        "    av_loss = np.mean(log_prob)\n",
        "#    print(type(av_loss)) # Ensure we are outputting a float\n",
        "    return av_loss\n",
        "\n",
        "  def backward(self,y_true):\n",
        "    # D\n",
        "    \"\"\"\n",
        "    Computes the initial error signal delta (gradient of C_E(y, sigma(z)) with\n",
        "    respect to the input of the softmax layer z\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_true : numpy array, type int, shape = (batch_size,)\n",
        "        True labels or targets.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    delta : numpy array of shape (batch_size, num_classes)\n",
        "        The gradient of L (average cross entropy loss) with respect to z\n",
        "    \"\"\"\n",
        "    y_pred = self.softmax(self.z)\n",
        "    batch_size, num_classes = y_pred.shape\n",
        "    y_one_hot = np.zeros_like(y_pred)\n",
        "    y_one_hot[np.arange(batch_size), y_true] = 1\n",
        "    delta = (y_pred - y_one_hot) / batch_size\n",
        "#    print(delta.shape) #Check shape of delta is (batch_size, num_classes)\n",
        "    return delta\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo2B8STG33tv"
      },
      "source": [
        "### E. Testing your SCE class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKIFaQ-44DLY"
      },
      "source": [
        "In order to test the validity of our functions softmax, cross_entropy and backward, we decided to use PyTorch to ensure that our functions were mathematically correct and producing the expected outputs. We first produced a set of sample data and evaluated it using our implementation and PyTorch's built-in functions. For each of these functions we compared the output produced by our code and PyTorch and found that our Softmax, Cross_entropy, and backwards functions matched with PyTorch's implementations. These outputs confirmed that our functions were mathematically correct and have been implemented as required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D0nGaYRoGWRG",
        "outputId": "a2a45da0-8e0f-41c7-db94-e04399aa8042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------- Softmax testing -------------\n",
            "My softmax: [[0.65900114 0.24243297 0.09856589]]\n",
            "Torch softmax: [[0.6590012  0.24243298 0.09856589]]\n",
            "Match: True\n",
            "------------- Cross Entropy testing -------------\n",
            "My Cross entropy: 0.4170300162778335\n",
            "Torch Cross entropy: 0.4170299470424652\n",
            "Match: True\n",
            "------------- Backward member testing -------------\n",
            "My Bacward member: [[-0.34099886  0.24243297  0.09856589]]\n",
            "Torch Backward member: [[-0.34099883  0.24243298  0.0985659 ]]\n",
            "Match: True\n"
          ]
        }
      ],
      "source": [
        "# Demonstration your code works\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "z_np = np.array([[2,1,0.1]])\n",
        "y_np = np.array([0])\n",
        "\n",
        "z_torch = torch.tensor(z_np, dtype=torch.float32, requires_grad = True)\n",
        "y_torch = torch.tensor(y_np, dtype=torch.long)\n",
        "\n",
        "loss_torch = F.cross_entropy(z_torch, y_torch, reduction='mean')\n",
        "loss_torch.backward()\n",
        "\n",
        "softmax_torch = F.softmax(z_torch,dim=1).detach().numpy()\n",
        "loss_torch_val = loss_torch.item()\n",
        "grad_torch = z_torch.grad.detach().numpy()\n",
        "\n",
        "sce = SCE()\n",
        "softmax_np = sce.softmax(z_np)\n",
        "loss_np = sce.cross_entropy(y_np, softmax_np)\n",
        "grad_np = sce.backward(y_np)\n",
        "\n",
        "print(\"------------- Softmax testing -------------\")\n",
        "print(\"My softmax:\", softmax_np)\n",
        "print(\"Torch softmax:\", softmax_torch)\n",
        "print(\"Match:\", np.allclose(softmax_np, softmax_torch, atol= 1e-6))\n",
        "\n",
        "print(\"------------- Cross Entropy testing -------------\")\n",
        "print(\"My Cross entropy:\", loss_np)\n",
        "print(\"Torch Cross entropy:\", loss_torch_val)\n",
        "print(\"Match:\", np.allclose(loss_np, loss_torch_val, atol= 1e-6))\n",
        "\n",
        "print(\"------------- Backward member testing -------------\")\n",
        "print(\"My Bacward member:\", grad_np)\n",
        "print(\"Torch Backward member:\", grad_torch)\n",
        "print(\"Match:\", np.allclose(grad_np, grad_torch, atol= 1e-6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VtSklZb59V6"
      },
      "source": [
        "# Q4. [4 marks] Implementing a ReLU layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM8TM4uv4Stq"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3NodL8VV5qF"
      },
      "source": [
        "In this question you will implement a ReLU layer. You have already done this in one form during your lab assignments via functions, the only difference here is that you will implement it as a class. Recall given a scalar z that $\\phi(z) := ReLU(z) = max(0,z).$ To apply ReLU to a tensor it is simply applied to each element of the tensor independently Your ReLU layer will have the following member functions.\n",
        "- A forward member function which applies ReLU to every element of the input tensor.\n",
        "- A backward member function which returns the error signal for the previous layer $\\delta_{new}$ given the error signal\n",
        "$\\delta$ from the layer above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "892hEMtI-U6G"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyxG0kAT-OtN"
      },
      "source": [
        "<ol type = \"A\">\n",
        "  <li> Complete the definition of the forward function for the ReLU class outlined below. You should take advantage of NumPy broadcasting and avoid for loops. Also, you will need to store the input to the forward pass in a member variable so that it can be used for the backward pass.</li>\n",
        "  <li> Although ReLU is not differentiable, it is differentiable everywhere other than at zero (it is almost everywhere differentiable).  As is standard in software packages, we define the gradient of ReLU at $0$ to be $0$.Given a scalar $z \\in \\mathbb{R}$, as per our standard notation let $a = \\phi(z) \\in \\mathbb{R}$ and $\\delta=\\frac{\\partial L}{\\partial a} \\in \\mathbb{R}$. In the cell under the title \"Derivative with respect to the input\" show that\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial z} =  \\delta \\mathbb{1}(z > 0).\n",
        "  $$\n",
        "  <li> Complete the definition of the backward function. Hint: you should be able to achieve this by mutating $\\delta$ directly, or a copy of it, using boolean indexing. Again avoid the use of for loops. </li>\n",
        "  <li> Describe and demonstrate your tests for your ReLU class in the space provided below.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmdkik0N7YCG"
      },
      "source": [
        "### B. Derivative with respect to the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE2QNsy67a8c"
      },
      "source": [
        "Using the chain rule we can compute the derivative with respect to the input as follows,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{d a}{d z} = \\delta \\cdot \\frac{d \\phi(z)}{dz}\n",
        "\\end{align}\n",
        "since $\\phi(z) = max(0,z)$ we have that,\n",
        "\\begin{align}\n",
        "\\frac{d \\phi(z)}{dz} =\n",
        "\\begin{cases}\n",
        "1,\\quad \\text{if} \\quad z>0 \\\\\n",
        "0,\\quad \\text{if} \\quad z<0 \\\\\n",
        "\\text{undefined} \\quad \\text{if} \\quad z = 0 \\\\\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "if we define the derivative of $z=0$ to be 0 for convenience, as it is standard in many deep learning frameworks, we can now write,\n",
        "\\begin{align}\n",
        "\\frac{d \\phi(z)}{dz} = \\mathbb{1}(z > 0)\n",
        "\\end{align}\n",
        "Substituting this back into our original equation gives,\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z} =  \\delta  \\mathbb{1}(z > 0)\n",
        "\\end{align}\n",
        "as required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9-3UxDZ_AgB"
      },
      "source": [
        "### A class for ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B_dZ238HjVby"
      },
      "outputs": [],
      "source": [
        "class ReLU:\n",
        "  \"\"\"\n",
        "  Class implementing a ReLU layer for a neural network.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.z = None # Store input during forward pass for the backward pass\n",
        "    # Has no model parameters\n",
        "\n",
        "  def forward(self, z):\n",
        "    \"\"\"\n",
        "    Applies the ReLU function element-wise to the input z, also stores the input\n",
        "    z for the backward pass.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    z : input numpy array\n",
        "        The input to the ReLU layer\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "        numpy array of with the same shape as z\n",
        "        z with ReLU applied elementwise\n",
        "    \"\"\"\n",
        "    self.z = z\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "  def backward(self, delta):\n",
        "    \"\"\"\n",
        "    Computes the error signal for the ReLU layer given the error signal\n",
        "    from the next layer.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    delta : numpy array\n",
        "        Error signal from the next layer\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    delta_new : numpy array of with the same shape as self.z_in\n",
        "        Error signal propagated to the previous layer\n",
        "    \"\"\"\n",
        "    delta[self.z <= 0] = 0\n",
        "    return delta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2zgc6kL_MEH"
      },
      "source": [
        "### D. Testing your ReLU class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S1XyQKd_QgZ"
      },
      "source": [
        "Below we have defined three sperate test to allow us to verify that the ReLU function is performing as expected. The first test allows us to confirm that if we provided the function with negative numbers these would change to zero and positive inputs pass through unchanged. The backward test allows us to check that only neurons whose inputs were positive during the forward pass actually receive any back‑propagated error signal, and any inputs that are at or below zero has the gradient completely blocked and doesn’t propagate backward. For the final test we decided to use random arrays to ensure that we are not relying on one example and that our function is checking every entry not just the few that we listed in our first function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tY3aJGwt_VSo",
        "outputId": "38273ed8-0c32-425e-bcee-067b0f2cc423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True forward([-1.   0.   3.  -2.   4.5]) expected [0.  0.  3.  0.  4.5], got [0.  0.  3.  0.  4.5]\n",
            "True backward on [-2  0  2  5] expected [0 0 1 1], got [0 0 1 1]\n",
            "no gradient leaked through non-positive z =  True\n"
          ]
        }
      ],
      "source": [
        "# Test that a mixture of both negative and positive inputs produces the appropriate output\n",
        "def test_relu_forward():\n",
        "    relu = ReLU()\n",
        "    z = np.array([-1, 0, 3, -2, 4.5])\n",
        "    out = relu.forward(z)\n",
        "    expected = np.array([0, 0, 3, 0, 4.5])\n",
        "    print(np.array_equal(out, expected), f\"forward({z}) expected {expected}, got {out}\")\n",
        "\n",
        "def test_relu_backward_simple():\n",
        "    relu = ReLU()\n",
        "    z = np.array([-2, 0, 2, 5])\n",
        "    relu.forward(z)\n",
        "    delta = np.ones_like(z)\n",
        "    delta_new = relu.backward(delta)\n",
        "    expected = np.array([0, 0, 1, 1])\n",
        "    print(np.array_equal(delta_new, expected), f\"backward on {z} expected {expected}, got {delta_new}\")\n",
        "\n",
        "def test_relu_backward_preserves_shape_and_dtype():\n",
        "    relu = ReLU()\n",
        "    z = np.random.randn(3,4).astype(np.float32)\n",
        "    relu.forward(z)\n",
        "    delta = (np.random.randn(3,4) * 5).astype(np.float32)\n",
        "    delta_new = relu.backward(delta)\n",
        "    mask = (z <= 0)\n",
        "    print(\"no gradient leaked through non-positive z = \",np.all(delta_new[mask] == 0))\n",
        "\n",
        "# Run tests\n",
        "test_relu_forward()\n",
        "test_relu_backward_simple()\n",
        "test_relu_backward_preserves_shape_and_dtype()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UceTMwGa_hKz"
      },
      "source": [
        "# Q5. [6 marks] Implementing a linear layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx9UzbWL_nz0"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KW2sRpVWBxr"
      },
      "source": [
        "Here we implement a linear layer, in particular, given an input matrix $z \\in \\mathbb{R}^{N \\times D}$, a weight matrix $w \\in \\mathbb{R}^{D \\times M}$ and a bias vector $b \\in \\mathbb{R}^M$ then the forward pass of the linear layer computes an output\n",
        "$$\n",
        "a = zw + b^T \\in \\mathbb{R}^{N \\times M}.\n",
        "$$\n",
        "Note that in the above we adopt standard NumPy broadcasting rules to perform the addition and we think of\n",
        "- $N$ as the size of the batch,\n",
        "- $D$ as the dimension (or degrees of freedom) of each input feature,\n",
        "- and $M$ as the width of our linear layer.\n",
        "\n",
        "Your linear layer class will have the following member functions.\n",
        "- An init method which initializes the weights and biases.\n",
        "- A forward member function which computes the output matrix $a$ as defined above.\n",
        "- A backward member function which returns the error signal for the previous layer $\\delta_{new}$ given the error signal $\\delta$ from the layer above. This member function should also store down as member variables the gradient (tensor of partial derivatives) of the loss with respect to the biases and weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhwSbODs_n7I"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RSB8k8NCfit"
      },
      "source": [
        "<ol type = \"A\">\n",
        "  <li> Complete the definition of the forward function for the linear class outlined below. Make sure to store the input during the forward pass in a member variable so that it can be used in the backward pass</li>\n",
        "  <li> In the space provided below (Derivative with respect to the biases), show that\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial b} = \\delta^T 1_N.\n",
        "  $$\n",
        "  </li>\n",
        "  <li> In the space provided below (Derivative with respect to the weights), show that\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial w} = z^T \\delta.\n",
        "  $$\n",
        "  </li>\n",
        " <li> In the space provided below (Derivative with respect to the inputs), show  that\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial z} = \\delta w^T\n",
        "  $$\n",
        "  </li>\n",
        "  <li> Complete the definition of the backward function. You will need to compute and store in member variables the gradient with respect to the weights and biases - you must call these member variables dw and db respectively.\n",
        "  <li> Describe and demonstrate your tests for the linear class in the space provided below.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPyzFsdWAeyx"
      },
      "source": [
        "### B. Derivative with respect to the biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACgByvz6LEdu"
      },
      "source": [
        "We can see that component-wise we have\n",
        "\\begin{align}\n",
        "a_{ij} = (zw)_{ij} + b_j\n",
        "\\end{align}\n",
        "and therefore\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial b_j} = \\sum^N_{i=1} \\sum^M_{k=1} \\frac{\\partial L}{\\partial a_{ik}} \\cdot \\frac{\\partial a_{ik}}{\\partial b_j}\n",
        "\\end{align}\n",
        "Notice that,\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial a_{ij}}{\\partial b_j} = 1\n",
        "\\end{align}\n",
        "\n",
        "and $\\partial a_{ik}/\\partial b_j$ = 0 when $k \\neq j$. Therefore by the chain rule we now have,\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial b_j} = \\sum^N_{i=1} \\sum^M_{k=1} \\frac{\\partial L}{\\partial a_{ik}} \\cdot \\frac{\\partial a_{ik}}{\\partial b_j}  = \\sum^N_{k=1}\\frac{\\partial L}{\\partial a_{ij}} \\cdot 1 = \\sum^N_{i=1} \\delta_{ij}\n",
        "\\end{align}\n",
        "In vector matrix form if $\\delta \\in \\mathbb{R}^{N \\times M}$ and $1_N \\in \\mathbb{R}^N$ is a vector of all ones, then the vector M with j-th entry of $\\sum_i \\delta_{ij}$ is,\n",
        "\\begin{align}\n",
        "\\delta^T 1_N\n",
        "\\end{align}\n",
        "therefore we have,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial b} = \\delta^T 1_N\n",
        "\\end{align}\n",
        "as required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC9VAjZFAnFv"
      },
      "source": [
        "### C. Derivative with respect to the weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuTU6sPcLFHX"
      },
      "source": [
        "firstly we can see that,\n",
        "\\begin{align}\n",
        "\\frac{\\partial a_{ij}}{\\partial w_{kj}} = z_{ik}\n",
        "\\end{align}\n",
        "and that it is zero when $k' \\neq k$. Therefore by the chain rule we have that\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial w_{kj}} = \\sum^N_{i=1}\\frac{\\partial L}{\\partial a_{ij}} \\cdot \\frac{\\partial a_{ij}}{\\partial w_{kj}} = \\sum^N_{i=1} \\delta_{ij} z_{ik}\n",
        "\\end{align}\n",
        "summing all these up for $k = 1,...,D$ and $j = 1,...,M$ gives the $D \\times X$ matrix,\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial w} = z^T \\delta.\n",
        "\\end{align}\n",
        "as required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_blcdJKsmJ"
      },
      "source": [
        "### D. Derivative with respect to the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7Dti0zyLFu-"
      },
      "source": [
        "We can see that,\n",
        "\\begin{align}\n",
        "\\frac{\\partial a_{ij}}{\\partial z_{ik}} = w_{kj}\n",
        "\\end{align}\n",
        "\n",
        "by the chain rule we now have,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_{ik}} = \\sum^M_{j=1} \\frac{\\partial L}{\\partial a_{ij}} \\frac{\\partial a_{ij}}{\\partial z_{ik}} = \\sum^M_{j=1} \\delta_{ij}w_{kj}\n",
        "\\end{align}\n",
        "summing all these up for $i = 1,...,N$ and $k = 1,...,D$ gives the matrix\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z} = \\delta w^T\n",
        "\\end{align}\n",
        "as required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROJ1slud_n_S"
      },
      "source": [
        "### A class for a linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iTIYbkAj8ggu"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  \"\"\"\n",
        "  Class implementing a linear layer for a neural network\n",
        "  \"\"\"\n",
        "  def __init__(self, in_features, out_features):\n",
        "\n",
        "    # Implement He-scaling\n",
        "    scale = np.sqrt(2/ in_features)\n",
        "    # Define parameters\n",
        "    self.w = (np.random.randn(in_features, out_features) * scale) # EXTRA: how could we scale this initialization better\n",
        "    self.b = np.zeros(out_features)\n",
        "    # Define gradient of parameters\n",
        "    self.dw = None\n",
        "    self.db = None\n",
        "    # Store input during forward pass for the backward pass\n",
        "    self.z = None\n",
        "\n",
        "\n",
        "  def forward(self, z):\n",
        "    # A\n",
        "    \"\"\"\n",
        "    For a given input tensor z computes z@w + b (take advantage of broadcasting!)\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "    z : input numpy array of shape (batch_size, in_features)\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "        numpy array of shape (batch_size, out_features)\n",
        "    \"\"\"\n",
        "    self.z = z\n",
        "    a = z @ self.w + self.b\n",
        "    return a\n",
        "\n",
        "  def backward(self, delta):\n",
        "    # E\n",
        "    \"\"\"\n",
        "    Computes the error signal for a layer given the error signal\n",
        "    from the next layer. Also computes / stores the gradient of the loss\n",
        "    with respect to the parameters of the layer, i.e., dw and db.\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "    delta : numpy array of shape (batch_size, out_features)\n",
        "        Error signal from the next layer\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    delta_new : numpy array of with the same shape as self.z\n",
        "        Error signal propagated to the previous layer\n",
        "    \"\"\"\n",
        "    self.dw = self.z.T @ delta\n",
        "    self.db = np.sum(delta, axis=0)\n",
        "    delta_new = delta @ self.w.T\n",
        "    return delta_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeKwGn-IAUYm"
      },
      "source": [
        "### F. Testing your linear layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkQ7CiGUAaA4"
      },
      "source": [
        "Below we designed tests to ensure our Linear layer functions were operating as expected. First we tested our forward pass by fixing W and b to known values and verified that forward(z) returned exactly $zw + b$. We additionally designed a test that checked the gradient produced by our backward  function. We accomplished this by sampling random weight and bias values, computing their gradients analytically via our backward method, estimating the same gradients numerically, and then comparing the two to observe any potential discrepancies. We found that there was practically no difference, confirming that our functions were working correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OGqQVzk6AThu",
        "outputId": "f7dbc313-0f84-4f93-c5f5-d4eafde8462d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directional check error: 1.921840464547131e-11\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Define parameters\n",
        "np.random.seed(0)\n",
        "N, D, M = 5, 4, 3\n",
        "z     = np.random.randn(N, D)\n",
        "delta = np.random.randn(N, M)       # partial L/ partial a\n",
        "U     = np.random.randn(D, M)       # direction for w\n",
        "V     = np.random.randn(M)          # direction for b\n",
        "eps   = 1e-5\n",
        "layer = Linear(D, M)\n",
        "\n",
        "# Test backward function\n",
        "out = layer.forward(z)\n",
        "dz = layer.backward(delta)\n",
        "dw_analytic, db_analytic = layer.dw, layer.db\n",
        "\n",
        "# Compute directional derivative\n",
        "D_analytic = np.sum(dw_analytic * U) + np.sum(db_analytic * V)\n",
        "\n",
        "# Define numeric directional derivative\n",
        "def loss_fn(w, b):\n",
        "    # loss = sum( a * delta )\n",
        "    a = z @ w + b\n",
        "    return np.sum(a * delta)\n",
        "\n",
        "# Apply numeric directional derivative\n",
        "D_plus  = loss_fn(layer.w + eps * U, layer.b + eps * V)\n",
        "D_minus = loss_fn(layer.w - eps * U, layer.b - eps * V)\n",
        "D_num   = (D_plus - D_minus) / (2 * eps)\n",
        "\n",
        "# Check both numericla and analytical match\n",
        "print(\"Directional check error:\", abs(D_num - D_analytic))\n",
        "print(abs(D_num - D_analytic) < 1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cno69BlM-TZ"
      },
      "source": [
        "# Q6. [20 marks] Implementing a 2D convolution layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKaqht6oPYLT"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVm9OqoqPaUy"
      },
      "source": [
        "We start by presenting the 2d-convolution operation (as used in machine learning) assuming a fixed stride of one and no dilation. Consider two four dimensional tensors $x \\in \\mathbb{R}^{N \\times C_{in} \\times H_{in} \\times V_{in}}$ and $u \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K_1 \\times K_2}$, where we assume $K_1 \\leq H_{in}$ and $K_2 \\leq V_{in}$. Let $H_{out}= H_{in} - K_1 +1$ and $V_{out}= V_{in} - K_2 +1$. The convolution of $x$ with $u$ produces a tensor $(x * u) \\in \\mathbb{R}^{N \\times C_{out} \\times H_{out} \\times V_{out}}$ whose entries are calculated as follows,\n",
        "$$\n",
        "(x*u)_{n,o,h,v} = \\sum_{c =0}^{C_{in}-1} \\sum_{i = 0}^{K_1-1} \\sum_{j=0}^{K_2-1} u_{o, c, i, j} x_{n,c, h+i, v+j},\n",
        "$$\n",
        "note in the above we use indexing starting from $0$ as is standard in Python. Note also that this is not a symmetric operation, indeed $x*u \\neq u*x$. We think of the lefthand argument $x$ as the *input* and the righthand argument $u$ as the *kernel* or *filter* which is applied to the input. A 2D convolution layer with stride 1, no dilation and square kernel of size $K$ convolves an input tensor $z \\in \\mathbb{R}^{N \\times C_{out} \\times H_{out} \\times V_{out}}$ with weights $w \\in R^{C_{out} \\times C_{in} \\times K \\times K}$ and adds a bias $b\\in R^{C_{out}}$ term to produce an output tensor\n",
        "$$\n",
        "a = (z * w) + b.\n",
        "$$\n",
        "Note that in the expression above we again adopt standard broadcasting rules to perform the addition, equivalently\n",
        "$$\n",
        "a_{n, o, h, v} = b_o + \\sum_{c=0}^{C_{in}-1} \\sum_{i =0}^{K-1} \\sum_{j =0}^{K-1} w_{o,c,i,j} z_{n, c, h+i, v+j}.\n",
        "$$\n",
        "Our goal is to implement a computationally efficiently class for a 2d convolutional layer. This class will have the following member functions or methods.\n",
        "- An init method for initializing the key member variables including the weights $w$ and biases $b$.\n",
        "- A conv2d operation which accepts an input tensor and a filter tensor and returns their 2d convolution. Note that we keep this seperate from our forward method as we will also be able to use it to compute gradients.\n",
        "- A forward method which computes and returns $a = (z * w) + b$.\n",
        "- A backward method which, given $\\delta := \\frac{\\partial L}{\\partial a}$ computes and returns $\\delta_{new} := \\frac{\\partial L}{ \\partial z}$. This method also computes and stores the parameter gradients $\\frac{\\partial L}{ \\partial w}$ and $\\frac{\\partial L}{ \\partial b}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Y25aFhjDLv"
      },
      "source": [
        "### Notation for processing 4 dimensional arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOxT-9isjDlB"
      },
      "source": [
        "We will use the following variables to denote the dimensions of our tensors.\n",
        "* $N$ is the size of the batch\n",
        "* $C_{in}$ is the number of input channels, $C_{out}$ is the number of output channels.\n",
        "* $H_{in}$ is the height of the input tensor, $H_{out}$ is the height of the output tensor.\n",
        "* $V_{in}$ is the width of the output tensor, $V_{out}$ is the width of the output tensor.\n",
        "* $K$ is the dimension of our kernel, recall for simplicity we are assuming our kernel size is fixed.\n",
        "\n",
        "Key tensors we will need to discuss are\n",
        "* The input tensor, $z \\in \\mathbb{R}^{N\\times C_{in} \\times H_{in} \\times V_{in}}$.\n",
        "* The output tensor, $a \\in \\mathbb{R}^{N \\times C_{out} \\times H_{out} \\times V_{out}}$.\n",
        "* The tensor of weights (one filter per input-output channel pair), $w \\in R^{C_{out} \\times C_{in} \\times  K \\times K}$.\n",
        "* The tensor of biases (one per output channel), $b \\in \\mathbb{R}^{C_{out}}$.\n",
        "* The error signal from the next layer $\\delta = \\frac{\\partial L}{ \\partial a} \\in \\mathbb{R}^{N \\times C_{out} \\times H_{out} \\times V_{out}}$.\n",
        "* The error signal to be propagated to the previous layer $\\delta_{new} = \\frac{\\partial L}{ \\partial z} \\in \\mathbb{R}^{N \\times C_{in} \\times H_{in} \\times V_{in}}$.\n",
        "\n",
        "Care is needed for what follows so here we establish some standard usages for the indices we will use.\n",
        "\n",
        "*   $n,m$ will be used as indices over the batch\n",
        "*   $o,p$ will be used as indices over the output channel\n",
        "*   $c,d$ will be used as indices over the input channel\n",
        "*   $i,j,k,l$ will be used as indices over the kernel dimension\n",
        "*   $g,h$ will be used as indices over the output height\n",
        "*   $v,u$ will be used as indices over the output width\n",
        "*   $q,r$ will be used as indices over the input height\n",
        "*   $s,t$ will be used as indices over the input width"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QflbiyijcMeU"
      },
      "source": [
        "### Questions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWiJ8UaV2eJS"
      },
      "source": [
        "<ol type = \"A\">\n",
        "  <li> Implement the 2d convolution operation $(x*u)_{n,o,h,v} = \\sum_{c =1}^{C_{in}} \\sum_{i = 0}^{K_1-1} \\sum_{j=0}^{K_2-1} u_{o, c, i, j} x_{n,o, h+i, v+j}$ as the member function conv2d of the Conv2D class defined below. Try to do this as efficiently as possible by minimizing the number of nested for loops. A simple implementation of conv2d requires 7 nested for loops, you can reduce this to two however if you are careful with broadcasting. </li>\n",
        "  <li> Describe your approach to implementing conv2d in detail in the text cell titled \"Description of 2d convolution operation\" below. Feel free to include and highlight particular snippets of code.\n",
        "  </li>\n",
        "  <li> Implement the forward member function of the Conv2D class in order to compute $a = (z * w) + b$. You should call the member function conv2d and supply it with the appropriate arguments. </li>\n",
        "  <li> In the cell marked \"Derivative of the loss with respect to the biases\" show that $\\frac{\\partial L}{\\partial b_p} = \\sum_{n=1}^N \\sum_{h=1}^{H_{out}} \\sum_{v = 1}^{V_{out}} \\delta_{n,p,h,v}$. Comment on how $\\frac{\\partial L}{\\partial b}$ can be computed efficiently with a single line of code and without using any for loops.</li>\n",
        "  <li> For a given four dimensional tensor $x$, let $x'$ denote a tensor with the same elements as $x$ but with its first and second axes swapped, i.e., $x_{a,b,c,d} = x'_{b,a,c,d}$. In the cell marked \"Derivative of the loss with respect to the weights\", show that $\\frac{\\partial L}{\\partial w} = (z'*\\delta)'$.</li>\n",
        "  <li> Let $\\hat{w}$ denote the flipped version of $w$ in the following sense: the output and input channel axes are swapped and the height and width indices are flipped, i.e., $w_{o,c,i,j} = \\hat{w}_{c,o,K-1-i, K-1-j}$. In addition, let $\\tilde{\\delta} \\in \\mathbb{R}^{N, C_{out}, H_{out}+K-1, V_{out} + K -1}$ denote the $K-1$ zero padded version of $\\delta$, where each height and channel dimension is padded with $K-1$ zeroes on either side. In the cell marked \"Derivative of the loss with respect to the inputs\" prove that $\\frac{\\partial L}{\\partial z} = \\tilde{\\delta}*\\hat{w}$.</li>\n",
        "  <li> Implement the backward method of the Conv2D class outlined below.</li>\n",
        "  <li> Describe your tests for your Conv2D class and show it passes them in the space provided below.\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trBwxokiNdL6"
      },
      "source": [
        "### B. Description of 2d convolution operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVVwBDHHOK-H"
      },
      "source": [
        "The aim of our conv2d method is to perform a 2d convolution between an input tensor $z$ and a weight tensor $w$ in order to calculate the following,\n",
        "\\begin{align}\n",
        "(x*u)_{n,o,h,v} = \\sum_{c =0}^{C_{in}-1} \\sum_{i = 0}^{K_1-1} \\sum_{j=0}^{K_2-1} u_{o, c, i, j} x_{n,c, h+i, v+j},\n",
        "\\end{align}\n",
        "We managed to achieve this using two nested python loops by using careful broadcasting over the kernel dimension i,j, and a single vectorized contraction over the input with dimension c.\n",
        "First we compute the height and width of the output tensor along with the number of output channels C_out as follows,\n",
        "\n",
        "    N, C_in, H_in, V_in = x.shape\n",
        "    C_out, _, K1, K2 = u.shape\n",
        "    H_out = H_in - K1 + 1\n",
        "    V_out = V_in - K2 + 1\n",
        "\n",
        "    conv_out = np.zeros((N, C_out, H_out, V_out))\n",
        "\n",
        "\n",
        "this is so we ensure a already has the correct output shape we desire which is $(N, C_{out}, H_{out}, V_{out})$\n",
        "\n",
        "Next we decided that rather than looping over each $(n,c,h,v)$ we noticed that for each fixed (i,j) in the $K \\times K$,\n",
        "\\begin{align}\n",
        "w_{o,c,i,j} z_{n, c, h+i, v+j}\n",
        "\\end{align}\n",
        "was applied identically across each batch $n$, input channel $c$, output height $h$, and output width $v$, thus we were able to build the nested loop as follows,\n",
        "\n",
        "    for i in range(K1):\n",
        "        for j in range(K2):\n",
        "            x_slice = x[:, :, i:i + h_out, j:j + V_out]\n",
        "            u_slice = u[:, :, i, j]\n",
        "            tmp = np.tensordot(x_slice, u_slice, axes=([1], [1]))\n",
        "            conv_out += tmp.transpose(0, 3, 1, 2)\n",
        "    return conv_out\n",
        "\n",
        "where x_slice selected all $z_{n, c, h+i,v+j}$ at once and w_slice contained all the current $w_{o,c,i,j}$. we were then able to use these to sum over $c$ using tensordot to offload the iterations over $n,h$ and $v$ to avoid python loops. By using this structure we were able to avoid the unnecessary use of loops and optimised our code, producing the final convolution of $z$ and $w$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaSJsh6GNdbh"
      },
      "source": [
        "### D. Derivative of the loss with respect to the biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqPALX8hPIZ8"
      },
      "source": [
        "we want to show that $\\frac{\\partial L}{\\partial b_p} = \\sum_{n=1}^N \\sum_{h=1}^{H_{out}} \\sum_{v = 1}^{V_{out}} \\delta_{n,p,h,v}$ to do this we will use the chain rule to solve,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial b_p} = \\sum_{n=1}^N \\sum_{h=1}^{H_{out}} \\sum_{v = 1}^{V_{out}} = \\frac{\\partial L}{\\partial a_{n,p,h,v}} \\cdot \\frac{\\partial a_{a,p,h,v}}{\\partial b_p}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "since we have that,\n",
        "\\begin{align}\n",
        "a_{n,p,h,v} = (z \\ast w)_{n,p,h,v} + b_p\n",
        "\\end{align}\n",
        "we can see that,\n",
        "\\begin{align}\n",
        "\\frac{\\partial a_{n,p,h,v}}{\\partial b_p} = 1\n",
        "\\end{align}\n",
        "furthermore since $L$ depends on $a$ we have that\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial a_{n,p,h,v}} = \\delta_{n,p,h,v}\n",
        "\\end{align}\n",
        "therefore we have that,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial b_p} = \\frac{\\partial L}{\\partial a_{n,p,h,v}} \\cdot \\frac{\\partial a_{a,p,h,v}}{\\partial b_p} = \\sum_{n=1}^N \\sum_{h=1}^{H_{out}} \\sum_{v = 1}^{V_{out}} \\delta_{n,p,h,v} \\cdot 1 = \\sum_{n=1}^N \\sum_{h=1}^{H_{out}} \\sum_{v = 1}^{V_{out}} \\delta_{n,p,h,v}\n",
        "\\end{align}\n",
        "as required.\n",
        "\n",
        "we can efficiently compute $\\frac{\\partial L}{\\partial b}$ without any python loops by summing up the 4 Dimensional delta tensor over the batch, height and width dimensions. The code would be as follows,\n",
        "\n",
        "db = delta.sum(axis=(0, 2, 3))\n",
        "\n",
        "since delta has shape $(N, C_{out}, H_{out}, V_{out})$ this code is summing over the batch, height and width dimensions as previously stated, avoiding any use of loops.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Zr2JJvNdmO"
      },
      "source": [
        "### E. Derivative of the loss with respect to the weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5TG3h3RPAUP"
      },
      "source": [
        "we need to show that,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial w_{o,c,i,j}} = \\sum_{n=0}^{N-1} \\sum_{h=0}^{H_{out}-1} \\sum_{v=0}^{V_{out}-1} \\delta'_{o,n,h,v} z'_{c,n,h+i,v+j}\n",
        "\\end{align}\n",
        "\n",
        "first we define our tensors with swapped axis',\n",
        "\\begin{align}\n",
        "z'_{c,n,h,v} = z_{n,c,h,v}, \\qquad  \\delta'_{o,n,h,v} = \\delta_{n,o,h,v}\n",
        "\\end{align}\n",
        "\n",
        "using out definition of 2d convolution we have,\n",
        "\\begin{align}\n",
        "(z' \\ast \\delta')_{c,o,i,j}=  \\sum_{n=0}^{N-1} \\sum_{h=0}^{H_{out}-1} \\sum_{v=0}^{V_{out}-1} \\delta'_{o,n,h,v} z'_{c,n,h+i,v+j}\n",
        "\\end{align}\n",
        "substituting the axis swapped $z'$ and $\\delta'$ gives,\n",
        "\\begin{align}\n",
        "(z' \\ast \\delta')_{c,o,i,j} = \\sum_{n,h,v} \\delta_{n,o,h,v} z_{n,c,h+1,v+j} = \\frac{\\partial L}{\\partial w_{o,c,i,j}}\n",
        "\\end{align}\n",
        "Swapping the axis' $(c,o)$ so that we can recover $(o,c,i,j)$ by utilising a transpose on these axis' gives,\n",
        "\\begin{align}\n",
        "(z' \\ast \\delta')_{c,o,i,j} = ((z' \\ast \\delta')')_{o,c,i,j} = \\frac{\\partial L}{\\partial w_{o,c,i,j}}\n",
        "\\end{align}\n",
        "This gives exactly,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial w} = (z' \\ast \\delta')'\n",
        "\\end{align}\n",
        "as required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR_7jTDiNdvz"
      },
      "source": [
        "### F. Derivative of the loss with respect to the input\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_B5kvF5N99I"
      },
      "source": [
        "first we begin my recalling that $\\delta_{n,o,h,v} = \\frac{\\partial L}{\\partial a_{n,o,h,v}}$. Then by the chain rule we have that,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_{n,c,q,r}} = \\sum_{o=0}^{C_{out}-1} \\sum_{h=0}^{H_{out}-1} \\sum_{v=0}^{V_{out}-1} \\delta_{n,o,h,v} \\frac{\\partial a_{n,o,h,v}}{\\partial z_{n,c,q,r}}\n",
        "\\end{align}\n",
        "since\n",
        "$$\n",
        "a_{n, o, h, v} = b_o + \\sum_{c=0}^{C_{in}-1} \\sum_{i =0}^{K-1} \\sum_{j =0}^{K-1} w_{o,c,i,j} z_{n, c, h+i, v+j}.\n",
        "$$\n",
        "we have that,\n",
        "\\begin{align}\n",
        "\\frac{\\partial a_{n,o,h,v}}{\\partial z_{n,c,q,r}} =\n",
        "\\begin{cases}\n",
        "w_{o,c,h,v}, \\quad \\text{if} \\quad  q= h+i \\space \\text{and} \\space r = v+j \\space \\text{for some} \\space 0 \\leq i, j < K \\\\\n",
        "0, \\quad \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "This is because a particular input $z_{n,c,q,r}$ only affects the activations $a_{n,o,h,v}$ whose kernel $(i,j)$ has exactly $(q,r)$.\n",
        "Computing the chain rule and plugging in $h=q-i$ and $v=r-j$ then gives,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_{n,c,q,r}} = \\sum_{o=0}^{C_{out}-1} \\sum_{i =0}^{K-1} \\sum_{j =0}^{K-1} \\delta_{n,o,q-i,r-j} \\space w_{o,c,i,j}\n",
        "\\end{align}\n",
        "Since we have that $\\tilde{\\delta} \\in \\mathbb{R}^{N, C_{out}, H_{out}+K-1, V_{out} + K -1}$ is the $K-1$ zero padded version of $\\delta$, where each height and channel dimension is padded with $K-1$ zeroes on either side. Then for all valid $(q,r)$ we have that,\n",
        "\\begin{align}\n",
        "\\tilde{\\delta}_{n, o, q-i+(K+1),r-j+(K+1)}\n",
        "\\begin{cases}\n",
        "\\delta_{n,o,q-i,r-j} \\quad \\text{if} \\space 0 \\leq q-i < H_{out}, 0 \\leq r-j < V_{out} \\\\\n",
        "0, \\quad \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "The gradient now becomes,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_{n,c,q,r}} = \\sum_{o=0}^{C_{out}-1} \\sum_{i =0}^{K-1} \\sum_{j =0}^{K-1} \\tilde{\\delta}_{n, o, q-i+(K+1),r-j+(K+1)} \\space w_{o,c,i,j}\n",
        "\\end{align}\n",
        "if we then change the index's as follows $i \\mapsto K-1-i, j \\mapsto K-1-j$ and let $\\hat{w}$ be the flipped kernel with,\n",
        "\\begin{align}\n",
        "\\hat{w}_{c,o,i,j} = w_{o,c,K-1-i,K-1-j}\n",
        "\\end{align}\n",
        "the sum then becomes,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_{n,c,q,r}} = \\sum_{o=0}^{C_{out}-1} \\sum_{i =0}^{K-1} \\sum_{j =0}^{K-1} \\tilde{\\delta}_{n, o, q+i,r+j} \\space w_{c,o,i,j}\n",
        "\\end{align}\n",
        "Using 2d convolution we can write,\n",
        "\\begin{align}\n",
        "(\\tilde{\\delta} \\ast \\hat{w})_{n,c,q,r} = \\sum_{o=0}^{C_{out}-1} \\sum_{i =0}^{K-1} \\sum_{j =0}^{K-1} \\hat{w}_{c,o,i,j} \\space \\hat{\\delta}_{n,o,q+i,r+j}\n",
        "\\end{align}\n",
        "and thus we have.\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z} = \\tilde{\\delta} \\ast \\hat{w}\n",
        "\\end{align}\n",
        "as required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOcNA-uNPlhO"
      },
      "source": [
        "### Conv2D Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tnYU7SDbjAMt"
      },
      "outputs": [],
      "source": [
        "class Conv2D:\n",
        "  \"\"\"\n",
        "  Class implementing a Conv2D layer for a neural network.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, C_in, C_out, K):\n",
        "    self.C_out = C_out\n",
        "    self.C_in = C_in\n",
        "    self.K=K\n",
        "    # EXTRA: can we initialize the weights better by scaling them appropriatly?\n",
        "    # Implement He-sclaing where in_features is = C_in x K x K\n",
        "    scale = np.sqrt(2 / (C_in * K * K))\n",
        "    self.w = (np.random.randn(self.C_out, self.C_in, self.K, self.K) * scale)\n",
        "    self.b = np.zeros(self.C_out)\n",
        "    self.dw = None\n",
        "    self.db = None\n",
        "    self.z = None\n",
        "\n",
        "  def conv2d(self, x, u):\n",
        "    \"\"\"\n",
        "    For a given input tensor z and filter u computes x*u where here * denotes the 2D\n",
        "    convolution.\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "    x : input numpy array of shape (N, C_in, H_in, V_in)\n",
        "    u : numpy array of shape (C_out, C_in, K1, K2)\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "        numpy array of shape (N, C_out, H_out, V_out)\n",
        "    \"\"\"\n",
        "    N, C_in, H_in, V_in = x.shape\n",
        "    C_out, _, K1, K2 = u.shape\n",
        "\n",
        "    # Compute size of correct convolution output\n",
        "    H_out = H_in - K1 + 1\n",
        "    V_out = V_in - K2 + 1\n",
        "\n",
        "    # Initialise the output tensor to zeros\n",
        "    conv_out = np.zeros((N, C_out, H_out, V_out))\n",
        "\n",
        "    # Iterate over each (i,j) position of the K1xK2 window\n",
        "    for i in range(K1):\n",
        "        for j in range(K2):\n",
        "            # Extract the portion of x that this element multiplies (N, C_in, H_out, V_out)\n",
        "            x_slice = x[:, :, i:i + H_out, j:j + V_out]\n",
        "\n",
        "            # Extract corresponding weights for output channels\n",
        "            u_slice = u[:, :, i, j]\n",
        "\n",
        "            # Compute tensordot over x_slice and u_slice resulting in shape (N, H_out, V_out, C_out)\n",
        "            tmp = np.tensordot(x_slice, u_slice, axes=([1], [1]))\n",
        "\n",
        "            # Reorder axes to get (N, C_out, H_out, V_out)\n",
        "            conv_out += tmp.transpose(0, 3, 1, 2)\n",
        "    return conv_out\n",
        "\n",
        "\n",
        "  def forward(self,z):\n",
        "    \"\"\"\n",
        "    For a given input tensor z computes z*self.w + self.b where here * denotes the 2D\n",
        "    convolution operation.\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "    z : input numpy array of shape (N, C_in, H_in, V_in)\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "        numpy array of shape (N, self.C_out, H_in - self.K + 1, V_in - self.K + 1)\n",
        "    \"\"\"\n",
        "    self.z = z\n",
        "    z_w = self.conv2d(z, self.w)\n",
        "    a = z_w + self.b[None, :, None, None]\n",
        "    return a\n",
        "\n",
        "  def backward(self, delta):\n",
        "    \"\"\"\n",
        "    Computes the error signal for a convolutional layer given the error signal\n",
        "    from the next layer. Also computes / stores the gradient of the loss\n",
        "    with respect to the parameters of the layer, i.e., updates self.dw and self.db\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "    delta : numpy array of shape (N, C_out, H_out, V_out)\n",
        "        Error signal from the next layer\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    delta_new : numpy array of shape (N, self.C_in, H_out+self.K-1, V_out+self.K-1)\n",
        "        Error signal for the previous layer\n",
        "    \"\"\"\n",
        "    # Delta\n",
        "    N, C_out, H_out, V_out = delta.shape\n",
        "\n",
        "    # Z\n",
        "    _, C_in, H_in, V_in = self.z.shape\n",
        "    K = self.K\n",
        "\n",
        "    # Compute bias gradient (C_out)\n",
        "    self.db = delta.sum(axis=(0, 2, 3))\n",
        "\n",
        "    # Compute weight gradient (C_out, C_in, K, K)\n",
        "    self.dw = np.zeros_like(self.w)\n",
        "\n",
        "    # For each (i,j) correlate delta with z\n",
        "    for i in range(K):\n",
        "        for j in range(K):\n",
        "\n",
        "            # (N, C_in, H_out, v_out)\n",
        "            z_slice = self.z[:, :, i:i+H_out, j:j+V_out]\n",
        "\n",
        "            # Tensordot over N, H_out, V_out to gain shape (C_out, C_in)\n",
        "            grad_ij = np.tensordot(delta, z_slice, axes=([0, 2, 3], [0, 2, 3]))\n",
        "            self.dw[:, :, i, j] = grad_ij\n",
        "\n",
        "    # Compute gradient w.r.t inputs\n",
        "    pad = K - 1\n",
        "    delta_pad = np.pad(delta,\n",
        "                      ((0,0), (0,0), (pad,pad), (pad,pad)), mode='constant', constant_values=0)\n",
        "    w_flip = self.w[:, :, ::-1, ::-1]\n",
        "    u = w_flip.transpose(1, 0, 2, 3)\n",
        "    delta_new = self.conv2d(delta_pad, u)\n",
        "    return delta_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SxL3lH1YROQ"
      },
      "source": [
        "### H. Testing your Conv2D class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqaZynaNYcQB"
      },
      "source": [
        "To test our Conv2D class we designed a variety of tests to ensure that the code was robust and functional. The first test we performed was to verified the shape of the weight and bias arrays to ensure that they were being set up with the correct dimensions. The second test we performed generated a random input and filter tensor and verified that using loops and tensordot in our conv2d function produced the same output as PyTorchs' 2d convolution. Next we decided to test the forward function and ensure that it was able to call conv2d and add the bias term ensuring that this output matched that of PyTorches'. When testing the backwards function we ensured that it copied the same weight and bias into a conv2d layer with PyTorch, we then ran a synthetic loss by multiplying its output with a random error signal and calling our backward function. After this we then compared the resulting weight bias and input gradients against those produced by our backward method to ensure that our function was performing as expected. Finally, we wrote an edge case test where the kernel size equalled the height and width dimensions of the input and verified the our forward pass still matched PyTorch's implementation. Together, these tests we computed covered initialisation, forward convolution, bias-addition,backward gradient computation and extreme corner cases. In all tests they were successful and as such lead us to confirm that the implementation of our code is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-3h6r6inYZx1",
        "outputId": "ec798e90-2e42-40c0-8140-021958c63d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Does w have the expeceted shape?  True\n",
            " Does b have the expeceted shape?  True\n",
            " Do both computations match for conv2d? True\n",
            " Does the forward function match the torch + bias computation? True\n",
            " Do the weight gradients from both the backward method and pytorch match? True\n",
            " Do the bias gradients from both the backward method and pytorch match? True\n",
            " Do the input gradients from both the backward method and pytorch match? True\n",
            " Does the edge cases computations match for conv2d? True\n"
          ]
        }
      ],
      "source": [
        "# Test that Conv2D makes weight and bias arrays with correct dimensions\n",
        "conv_np = Conv2D(3,2,3)\n",
        "print(f\" Does w have the expeceted shape? \",conv_np.w.shape == (2, 3, 3, 3))\n",
        "print(f\" Does b have the expeceted shape? \", conv_np.b.shape == (2,))\n",
        "\n",
        "# Test that our conv2d function produces the same output as torch.conv2d\n",
        "x_np = np.random.randn(4, 3, 8, 8)\n",
        "u_np = np.random.randn(2, 3, 3, 3)\n",
        "out_np = conv_np.conv2d(x_np, u_np)\n",
        "out_t = fun.conv2d(torch.tensor(x_np, dtype=torch.float),torch.tensor(u_np, dtype=torch.float)).numpy()\n",
        "print(f\" Do both computations match for conv2d?\", np.allclose(out_np, out_t, atol=1e-5))\n",
        "\n",
        "# Test that our forward function matches the torch + bias computation\n",
        "conv_np.w = u_np.copy()\n",
        "conv_np.b = np.random.randn(2)\n",
        "out_f_np = conv_np.forward(x_np)\n",
        "out_f_t = (fun.conv2d(torch.tensor(x_np, dtype=torch.float), torch.tensor(u_np, dtype=torch.float), bias=torch.tensor(conv_np.b,dtype=torch.float)).numpy())\n",
        "print(f\" Does the forward function match the torch + bias computation?\", np.allclose(out_f_np, out_f_t, atol=1e-5))\n",
        "\n",
        "# Test our backward function against torches autograd\n",
        "conv_t = nn.Conv2d(3, 2, 3)\n",
        "with torch.no_grad():\n",
        "    conv_t.weight.copy_(torch.tensor(u_np, dtype=torch.float))\n",
        "    conv_t.bias.copy_(torch.tensor(conv_np.b, dtype=torch.float))\n",
        "\n",
        "x_t = torch.tensor(x_np, dtype=torch.float, requires_grad = True)\n",
        "delta_np = np.random.randn(*out_f_np.shape)\n",
        "delta_t = torch.tensor(delta_np, dtype=torch.float)\n",
        "\n",
        "y_t = conv_t(x_t)\n",
        "(y_t * delta_t).sum().backward()\n",
        "\n",
        "dw_exp = conv_t.weight.grad.numpy()\n",
        "db_exp = conv_t.bias.grad.numpy()\n",
        "dx_exp = x_t.grad.numpy()\n",
        "\n",
        "conv_np.z = x_np\n",
        "dx_np = conv_np.backward(delta_np)\n",
        "print(f\" Do the weight gradients from both the backward method and pytorch match?\", np.allclose(conv_np.dw, dw_exp, atol=1e-5))\n",
        "print(f\" Do the bias gradients from both the backward method and pytorch match?\", np.allclose(conv_np.db, db_exp, atol=1e-5))\n",
        "print(f\" Do the input gradients from both the backward method and pytorch match?\", np.allclose(dx_np, dx_exp, atol=1e-5))\n",
        "\n",
        "# Run a test at the edge case when K=H=W for further validity\n",
        "x_edge = np.random.randn(2, 3, 5, 5)\n",
        "conv_np = Conv2D(3, 2, 5)\n",
        "conv_np.w = np.random.randn(2, 3, 5, 5)\n",
        "conv_np.b = np.random.randn(2)\n",
        "out_e_np = conv_np.forward(x_edge)\n",
        "out_e_t = fun.conv2d(torch.tensor(x_edge, dtype=torch.float), torch.tensor(conv_np.w, dtype=torch.float), bias = torch.tensor(conv_np.b, dtype=torch.float)).numpy()\n",
        "print(f\" Does the edge cases computations match for conv2d?\", np.allclose(out_e_np, out_e_t, atol = 1e-5) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2z2PClD8cix"
      },
      "source": [
        "# Q7. [14 marks] Implementing max-pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m04gCZUOGt8Q"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOlZJySGHjfB"
      },
      "source": [
        "Max pooling is a fundamental operation used in CNNs which downsamples feature tensors. For ease we will not consider max-pooling with a dilation, but will incorporate a stride. To define the max-pooling operation as commonly used in deep learning, consider an input tensor $z \\in \\mathbb{R}^{N \\times C \\times H_{in} \\times V_{in}}$, let $K \\in \\mathbb{N}$ be the window (or kernel) dimension and $S \\in \\mathbb{N}$ be the stride. With $H_{out} = \\frac{H_{in} - K}{S} + 1$, $V_{out} = \\frac{V_{in} - K}{S} + 1$, then the elements of $a = MaxPool2D(z) \\in \\mathbb{R}^{N \\times C \\times H_{out} \\times V_{out}}$ are defined as\n",
        "$$\n",
        "a_{n,c,h,v} = \\max_{i, j \\in [0,K-1]} \\{ z_{n,c,Sh+i, Sv+j} \\}.\n",
        "$$\n",
        "In short, for each pair of batch and channel indices, MaxPool2D i) slides  a window of size $K \\times K$ over the $H_{in} \\times V_{in}$ plane skipping $S$ positions at a time, and ii), in each position selects the largest element in the window. *Note, again for ease we will write our code under the assumption that our input tensor shape, kernel size and stride should always be such that $\\frac{H_{in}- K}{S}$ and $\\frac{V_{in}- K}{S}$ are integers. You will need to check this when it comes to deploying your maxpooling layer in a network.*\n",
        "\n",
        "\n",
        "Our goal is to implement a max pooling layer and to this end we will build a class with the following member functions.\n",
        "- An init method which defines the size of the sliding window.\n",
        "- A forward member function which returns the Maxpool2D array given an input tensor and stores the locations of the maximum values of each patch for the backward pass.\n",
        "- A backward member function which returns the error signal for the previous layer $\\delta_{new}$ given the error signal from the layer above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIUkr_l1kSMg"
      },
      "source": [
        "### Questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3AojRdpskgB"
      },
      "source": [
        "<ol type = \"A\">\n",
        "  <li> For a given $n$ and $c$ assume $z_{n,c, Sh+i, Sv +j } = z_{n,c, Sh+k, Sv +l }$ if and only if $i=k$ and $j=l$. In addition, define\n",
        "  $$\n",
        "  (q(h,v), t(h,v)) = argmax_{i \\in [Sh, Sh + K-1], j \\in [Sv, Sv + K-1]}  z_{n,o,i,j} .\n",
        "  $$\n",
        "  In the text cell marked \"A. Derivative of loss with respect to input\" show that\n",
        "  $$\n",
        "  \\frac{\\partial L}{\\partial z_{m, c, r, s}} = \\sum_{h = 0}^{H_{out}-1} \\sum_{v = 0}^{V_{out}-1}\\frac{\\partial L}{\\partial a_{m,c,h,v}}\\mathbb{1}(r = q(h,v), s = t(h,v))\n",
        "  $$\n",
        "  In addition write a short paragraph commenting on i) what happens if two elements of the same patch are equal, in particular, is maxpool differentiable in this setting? ii) What are the practical implications of i) for backpropagation and iii) what information needs to be stored during the runtime of the forward member function in order to be able to run the backward member function?\n",
        "  <li> Complete the forward member function of the MaxPool2D class below. Your implementation should use no more than four nested for loops. </li>\n",
        "  <li> Implement the backward member function of the MaxPool2D class below. Note, if there are ties choose the element with the smallest flattened index. Again, your implementation should use no more than four nested for loops.</li>\n",
        "  <li> Describe your tests and demonstrate that your Maxpooling2D layer works </li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asOMoVcXkNyk"
      },
      "source": [
        "### A. Derivative of loss with respect to input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP9yygXAKUKS"
      },
      "source": [
        "We first start with the fact that,\n",
        "\\begin{align}\n",
        "a_{n,c,h,v} = \\max_{i, j \\in [0,K-1]} \\{ z_{n,c,Sh+i, Sv+j} \\}.\n",
        "\\end{align}\n",
        "Using the chain rule we want to find,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_{m,c,r,s}} = \\sum_{h = 0}^{H_{out}-1} \\sum_{v = 0}^{V_{out}-1}\\frac{\\partial L}{\\partial a_{m,c,h,v}} \\frac{\\partial a_{m,c,h,v}}{\\partial z_{m,c,r,s}}\n",
        "\\end{align}\n",
        "By definition of the max over the finite set let\n",
        "\\begin{align}\n",
        "(q(h,v), t(h,v)) = argmax_{i \\in [Sh, Sh + K-1], j \\in [Sv, Sv + K-1]}  z_{m,c,i,j}\n",
        "\\end{align}\n",
        "Then for each window $(h,v)$ we have\n",
        "\\begin{align}\n",
        "a_{m,c,h,v} = z_{m,c, q(h,v), t(h,v)}\n",
        "\\end{align}\n",
        "Computing the partial derivative then gives,\n",
        "\\begin{align}\n",
        "\\frac{\\partial a_{m,c,h,v}}{\\partial z_{m,c,r,s}}\n",
        "\\begin{cases}\n",
        "1, \\quad \\text{if} \\space (r,s) = (q(h,v), \\space t(h,v)) \\\\\n",
        "0, \\quad \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "This is simply an indicator function so plugging this into our chain rule gives,\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial z_{m,c,r,s}} = \\sum_{h = 0}^{H_{out}-1} \\sum_{v = 0}^{V_{out}-1}\\frac{\\partial L}{\\partial a_{m,c,h,v}}\\mathbb{1}(r = q(h,v), s = t(h,v))\n",
        "\\end{align}\n",
        "as required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7dnbX8rqptb"
      },
      "source": [
        "When two elements of the same patch are equal the max function no longer has a unique $argmax$ and as such the gradient is no longer well-defined. Because of this the Max-pooling is not differentiable when the two elements of the same patch are equal, this is because the function will have a non-differentiable corner making it no longer smooth. The practical implications for back-propagation allow the unique routing of gradients even though mathematically they are non-differentiable when elements of the same patch are equal. We do this by picking one location per window so that each $\\frac{\\partial L}{\\partial a_{m,c,h,v}}$ is routed to exactly one $(r,s)$. Additionally, since max-pooling only copies a single gradient per window, it neither amplifies nor diminishes the gradient magnitude beyond the one value it produces. During the runtime of the forward member function in order to be able to run the backward member function we need to be able to store the $argmax$ indicies for each output $(n,c,h,v)$ with the input coordinates $(r,s)$, so we store the achieved maximum in that window $(N,C,H_{out},W_{out})$. We also need to store the original $(N,C,H_{in},W_{in})$ so that we can compute the backward-gradient tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LvLsvIUKJWH"
      },
      "source": [
        "### MaxPool2D class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8TLHNM8i0Jwv"
      },
      "outputs": [],
      "source": [
        "class MaxPool2D:\n",
        "  \"\"\"\n",
        "  Class implementing a ReLU layer for a neural network.\n",
        "  \"\"\"\n",
        "  def __init__(self, K, S):\n",
        "    \"\"\"\n",
        "    Initializes the class variables, namely kernel / window size and stride\n",
        "    Parameters:\n",
        "    -----------\n",
        "    K : int\n",
        "        Size of the sliding window\n",
        "    S : int\n",
        "        Stride\n",
        "    \"\"\"\n",
        "    self.K = K\n",
        "    self.S = S\n",
        "\n",
        "  def forward(self, z):\n",
        "    # B.\n",
        "    \"\"\"\n",
        "    For a given input tensor z computes maxpooling of z with size window size KxK\n",
        "    with stride S.\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "    z : input numpy array of shape (N, C, H_in, H_out). For ease it is assumed that\n",
        "    (H_in - K )/S and (V_in - K )/S are integers.\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "        numpy array of shape (N, C, H_out, W_out)\n",
        "    \"\"\"\n",
        "    N, C, H_in, W_in = z.shape\n",
        "    K, S = self.K, self.S\n",
        "    H_out = int(((H_in - K) / S) + 1)\n",
        "    W_out = int(((W_in - K) / S) + 1)\n",
        "\n",
        "    # Check that that $\\frac{H_{in}- K}{S}$ and $\\frac{V_{in}- K}{S}$ are integers.\n",
        "    if (H_in - self.K) % self.S != 0 or (W_in - self.K) % self.S != 0:\n",
        "        raise ValueError(\n",
        "        f\"Invalid shape: (H_in - K) and (W_in - K) must be divisible by stride S. \"\n",
        "        f\"Received H_in={H_in}, W_in={W_in}, K={self.K}, S={self.S}\"\n",
        "    )\n",
        "\n",
        "    a = np.zeros((N, C, H_out, W_out))\n",
        "    self.max_indices = np.zeros((N, C, H_out, W_out, 2))\n",
        "    self.input_shape = z.shape\n",
        "\n",
        "    # Four nested loops over the batch-n , channel-c, output height-h and output width-w\n",
        "    for n in range(N):\n",
        "        for c in range(C):\n",
        "            for h in range(H_out):\n",
        "                for w in range(W_out):\n",
        "                    hs = h * S\n",
        "                    ws = w * S\n",
        "                    window = z[n, c, hs:hs + K, ws:ws + K]\n",
        "                    # Find max in KxK window\n",
        "                    flat_idx = np.argmax(window)\n",
        "                    i, j = np.unravel_index(flat_idx, (K, K))\n",
        "                    a[n, c, h, w] = window[i, j]\n",
        "                    # Store values of indices for the max\n",
        "                    self.max_indices[n, c, h, w] = (hs + i, ws + j)\n",
        "\n",
        "    return a\n",
        "\n",
        "  def backward(self, delta):\n",
        "    # C.\n",
        "    \"\"\"\n",
        "    Computes the error signal for a layer given the error signal\n",
        "    from the next layer. FOr ease it is assumed that\n",
        "    (H_in - K )/S and (V_in - K )/S are integers.\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "    delta : numpy array of shape (N, C, H_out, V_out)\n",
        "        Error signal from the next layer\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    delta_new : numpy array of shape (N, C, H_in, V_in)\n",
        "        Error signal propagated to the previous layer\n",
        "    \"\"\"\n",
        "    # Get stored values\n",
        "    N, C, H_in, W_in = self.input_shape\n",
        "    _, _, H_out, W_out = delta.shape\n",
        "\n",
        "    # Initialise gradient for previous layer\n",
        "    delta_prev = np.zeros((N, C, H_in, W_in))\n",
        "\n",
        "    # Four nested loops for batch - n, channel - c, output height - h, output width - w\n",
        "    for n in range(N):\n",
        "        for c in range(C):\n",
        "            for h in range(H_out):\n",
        "                for w in range(W_out):\n",
        "                    # put gradient in stored max position\n",
        "                    r, s = self.max_indices[n, c, h, w]\n",
        "                    r, s = int(r), int(s)\n",
        "                    delta_prev[n, c, r, s] += delta[n, c, h, w]\n",
        "\n",
        "    return delta_prev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7rRlDuZdL1P"
      },
      "source": [
        "### D. Testing your MaxPooling2D layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ8nHW4NdjUA"
      },
      "source": [
        "To test our maxpooling2d layer, we first tested our forward pass by creating a random input of shape and putting it in both our forward function and PyTorch's max_pool2d function with the same $K = 2, S = 2$. By displaying the maximum absolute difference between these two outputs we were able to verify that our implementation of the forward pass was correct as it aligned with PyTorch's implementation. Next we tested the backward function by taking our forward output and assuming a forward gradient of all ones. Our backward function then computed the gradient with respect to the input $z$ and compared this to the PyTorch implementation. After comparing the maximum absolute values once again, we saw no difference in our backward function and PyTorch's implementation, confirming that our backward-pass routing is correct. The final test we did checked that when all values were the same in a 2x2 window that our code sent the gradient to the first 'flat' index (the top-left corner of each window). Printing the map of the gradient showed that exactly one '1' per window were in the expected positions, demonstrating that our functions were able to deal with elements of the same patch whey they were equal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GElkA3Xtdg4B",
        "outputId": "a38dcd74-6c79-4edd-ddfe-ca98c3fc85c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max forward difference 0.0\n",
            "Max backwards difference 0.0\n",
            "Backward o all-ones patch [[1. 0. 1. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [1. 0. 1. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# Forward function testing\n",
        "\n",
        "z_np = np.random.rand(2, 3, 6, 6)\n",
        "z_torch = torch.tensor(z_np, requires_grad = True)\n",
        "pool_np = MaxPool2D(K=2, S=2)\n",
        "out_np = pool_np.forward(z_np)\n",
        "out_torch = fun.max_pool2d(z_torch, kernel_size = 2, stride = 2)\n",
        "\n",
        "print(\"Max forward difference\", np.max(np.abs(out_np - out_torch.detach().numpy())))\n",
        "\n",
        "#Backward function testing\n",
        "grad_back = np.ones_like(out_np)\n",
        "grad = pool_np.backward(grad_back)\n",
        "out_torch.sum().backward()\n",
        "grad_torch = z_torch.grad.numpy()\n",
        "\n",
        "print(\"Max backwards difference\", np.max(np.abs(grad - grad_torch)))\n",
        "\n",
        "# Equal value test\n",
        "z_equal = np.ones((1, 1, 4, 4))\n",
        "pool_equal = MaxPool2D(K=2, S=2)\n",
        "out_equal = pool_equal.forward(z_equal)\n",
        "grad_equal = pool_equal.backward(np.ones_like(out_equal))\n",
        "print(\"Backward o all-ones patch\", grad_equal[0,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-e2b3tp9AkD"
      },
      "source": [
        "# Q8. [6 marks] Train your scratch built CNN on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke-BlMiCfCVJ"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alXx9MYafEM3"
      },
      "source": [
        "Now that we have built classes for the various layers typically used in a CNN, we can test them together by building and training a network to on CIFAR10. To this end we are going to need\n",
        "- a gradient descent or GD class, this class has\n",
        "  - a learning rate member variable.\n",
        "  - A model member variable which allows access to the parameters we want to update.\n",
        "  - An init member function to initialize the two member variables described above.\n",
        "  - A step member function which applies one step of gradient descent to the parameters of the model.\n",
        "- A train member function which performs stochastic gradient descent to train a model.\n",
        "\n",
        "A simple CNN class to get you started is provided below but feel free to make changes to it. Do make sure to read the definition of this CNN class carefully. Finally, you should not be surprised if your code is significantly slower than Pytorch. If you struggle to train on the full CIFAR10 training dataset, choose a smaller subset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUUB8o6Di9dO"
      },
      "source": [
        "### A simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "E00WClt8qptc"
      },
      "outputs": [],
      "source": [
        "class CNN_simple:\n",
        "    def __init__(self):\n",
        "        # Block 1: 3→6 channels, 3×3 conv → ReLU → 2×2 pool\n",
        "        self.conv1 = Conv2D(3,  16, 5)\n",
        "        self.relu1 = ReLU()\n",
        "        self.pool1 = MaxPool2D(2, 2)\n",
        "\n",
        "        # Block 2: 6→12 channels, 3×3 conv → ReLU → 2×2 pool\n",
        "        self.conv2 = Conv2D(16,  32, 3)\n",
        "        self.relu2 = ReLU()\n",
        "        self.pool2 = MaxPool2D(2, 2)\n",
        "\n",
        "        # Block 3: 12→64 channels, 3×3 conv → ReLU → 2×2 pool\n",
        "        self.conv3 = Conv2D(32, 64, 3)\n",
        "        self.relu3 = ReLU()\n",
        "        self.pool3 = MaxPool2D(2, 2)\n",
        "\n",
        "        # Softmax\n",
        "        self.soft = SCE()\n",
        "\n",
        "        # Dummy forward pass to infer flatten size\n",
        "        dummy = np.zeros((1, 3, 32, 32))\n",
        "\n",
        "        h = self.pool1.forward(self.relu1.forward(self.conv1.forward(dummy)))\n",
        "        h = self.pool2.forward(self.relu2.forward(self.conv2.forward(h)))\n",
        "        h = self.pool3.forward(self.relu3.forward(self.conv3.forward(h)))\n",
        "        _, C, H, W = h.shape\n",
        "        self._feat_size = C * H * W\n",
        "\n",
        "        # Final fully‐connected head\n",
        "        self.fc = Linear(self._feat_size, 10)\n",
        "\n",
        "        # Tell the optimizer about every layer with parameters\n",
        "        self.param_layers = [self.conv1, self.conv2, self.conv3, self.fc]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1.forward(self.relu1.forward(self.conv1.forward(x)))\n",
        "        x = self.pool2.forward(self.relu2.forward(self.conv2.forward(x)))\n",
        "        x = self.pool3.forward(self.relu3.forward(self.conv3.forward(x)))\n",
        "\n",
        "        # Stash shape for backward\n",
        "        N, C, H, W = x.shape\n",
        "        self._current_shape = (N, C, H, W)\n",
        "\n",
        "        # Flatten and classify\n",
        "        x = x.reshape(N, C*H*W)\n",
        "        y = self.fc.forward(x)\n",
        "        return self.soft.softmax(y)\n",
        "\n",
        "    def loss(self, y_true, y_pred):\n",
        "        return self.soft.cross_entropy(y_true, y_pred)\n",
        "\n",
        "    def backward(self, y_true):\n",
        "        # Gradient of loss w.r.t y_true\n",
        "        delta = self.soft.backward(y_true)\n",
        "        # Head backward\n",
        "        delta = self.fc.backward(delta)\n",
        "\n",
        "        # Reshape back into feature‐map\n",
        "        N, C, H, W = self._current_shape\n",
        "        delta = delta.reshape(N, C, H, W)\n",
        "\n",
        "        # Invert block 3\n",
        "        delta = self.pool3.backward(delta)\n",
        "        delta = self.relu3.backward(delta)\n",
        "        delta = self.conv3.backward(delta)\n",
        "\n",
        "        # Invert block 2\n",
        "        delta = self.pool2.backward(delta)\n",
        "        delta = self.relu2.backward(delta)\n",
        "        delta = self.conv2.backward(delta)\n",
        "\n",
        "        # Invert block 1\n",
        "        delta = self.pool1.backward(delta)\n",
        "        delta = self.relu1.backward(delta)\n",
        "        delta = self.conv1.backward(delta)\n",
        "\n",
        "        return delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5rBKYmWMXdI"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbhxavtfiJfb"
      },
      "source": [
        "The following code will download CIFAR10 onto the colab virtual machine that you are using and store it in Numpy arrays (note we are not using Pytorch here so aren't going to use the dataloader as we did in Part I). Once you have run the cells below you should see a new folder `cifar-10-batches-py` appear in the folder on the left-hand-side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "E4su_fdOREFk",
        "outputId": "af1979c1-f8f7-40c5-f489-e07d729017fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n",
            "Extraction complete.\n"
          ]
        }
      ],
      "source": [
        "# URL of the CIFAR-10 (Python version) tarball\n",
        "url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "file_name = \"cifar-10-python.tar.gz\"\n",
        "\n",
        "# Download the dataset if it doesn't already exist\n",
        "if not os.path.exists(file_name):\n",
        "    print(f\"Downloading {file_name} from {url}...\")\n",
        "    urllib.request.urlretrieve(url, file_name)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "# Extract the tarball\n",
        "print(\"Extracting files...\")\n",
        "with tarfile.open(file_name, \"r:gz\") as tar:\n",
        "    tar.extractall()\n",
        "print(\"Extraction complete.\")\n",
        "\n",
        "# After extraction, the folder 'cifar-10-batches-py' should appear in your current directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdNkrqodMjqF"
      },
      "source": [
        "Clicking on the 'cifar-10-batches-py' folder you should see the following.\n",
        "\n",
        "\n",
        "1.   Five `data_batch_x` files: these are pickle (Python) files which store 10,000 images intended for training as well as their labels.\n",
        "2.   A `test_batch` pickle Python file: again, this stores another 10,000 images and their labels and is intended for testing your model once it has been trained\n",
        "3.  The other files you can ignore!\n",
        "\n",
        "The following cell passes through the five `data_batch_i` files, unpickles them and appends them into a list which is then converted into a NumPy array.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xFqWy6gumUwW"
      },
      "outputs": [],
      "source": [
        "# You don't need to change any code in this cell!\n",
        "train_images = []\n",
        "train_labels = []\n",
        "data_dir = \"cifar-10-batches-py\"\n",
        "test_batch = os.path.join(data_dir, \"test_batch\")\n",
        "\n",
        "for batch_idx in range(1, 6):  # 1..5\n",
        "    batch_filename = os.path.join(data_dir, f\"data_batch_{batch_idx}\")\n",
        "    with open(batch_filename, 'rb') as f:\n",
        "        batch_dict = pickle.load(f, encoding='latin1')\n",
        "        # 'data' is a 10000 x 3072 array of uint8\n",
        "        # 'labels' is a list of size 10000\n",
        "        images = batch_dict['data']\n",
        "        labels = batch_dict['labels']\n",
        "\n",
        "        train_images.append(images)\n",
        "        train_labels.extend(labels)\n",
        "\n",
        "\n",
        "batch_filename = os.path.join(data_dir, f\"test_batch\")\n",
        "with open(batch_filename, 'rb') as f:\n",
        "        batch_dict = pickle.load(f, encoding='latin1')\n",
        "        # 'data' is a 10000 x 3072 array of uint8\n",
        "        # 'labels' is a list of size 10000\n",
        "        test_images = batch_dict['data']\n",
        "        test_labels = batch_dict['labels']\n",
        "\n",
        "X_train = np.concatenate(train_images, axis=0)\n",
        "X_test = np.array(test_images)\n",
        "\n",
        "# Reshape input features so no longer flattened\n",
        "X_train = np.reshape(X_train, (50000, 3, 32, 32)).astype(np.float32)/255.0\n",
        "X_test = np.reshape(X_test, (10000, 3, 32, 32)).astype(np.float32)/255.0\n",
        "y_train = np.array(train_labels).astype(np.uint8)\n",
        "y_test = np.array(test_labels).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AzrBKbQqkBe"
      },
      "source": [
        "Let us inspect what the shape of these four numpy arrays is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BQfbtH4aolUP",
        "outputId": "d0140064-d2ff-44e7-a447-888a19f0deb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((50000, 3, 32, 32), (50000,), (10000, 3, 32, 32), (10000,))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZQP3gKZl7nd"
      },
      "source": [
        "We have so far loaded the data into numpy arrays. We now can visualize some of the images by running the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sxtiSLFCq98F",
        "outputId": "a56a0c05-ade5-4a4f-d12b-4a31868fa1c8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ind = 0\n",
        "plt.imshow(np.transpose(X_train[ind], (1, 2, 0))) # Transpose the image to (height, width, channels)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oi91vf9hOD6"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYsdPCTxhOSl"
      },
      "source": [
        "<ol type = \"A\">\n",
        "  <li> Complete the step member function of the GD class outlined below. This function should iterate through layers of the the model which have parameters and do a single step of gradient descent on both the weights and biases.  </li>\n",
        "  <li> Complete the train function outlined below as per the comments.</li>\n",
        "  <li> Try to create and train an instance of CNN_simple to fit CIFAR10. Experiment with changing the architecture of CNN_simple and also with doing some data preprocessing, e.g., normalisation. Comment on how your model performs and also on your observations with regard to the computational efficiency of your model versus the Pytorch implementation from Part I.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zblw7Y27hfEi"
      },
      "source": [
        "### A. implemeting a class for gradient descent (GD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ozBlsG3i_YHt"
      },
      "outputs": [],
      "source": [
        "class GD:\n",
        "  \"\"\"\n",
        "  Class implementing gradient descent for a neural network.\n",
        "  \"\"\"\n",
        "  def __init__(self, model, lr):\n",
        "    self.model = model\n",
        "    self.lr = lr\n",
        "\n",
        "  def step(self):\n",
        "    # A.\n",
        "    \"\"\"\n",
        "    Performs a single step of gradient descent for all parameters of the model.\n",
        "    This function should iterate through layers of the the model which have parameters\n",
        "    and do a single step of gradient descent on both the weights and biases.\n",
        "\n",
        "    Inputs:\n",
        "    -----------\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    ----------\n",
        "    None\n",
        "    \"\"\"\n",
        "    for layer in self.model.param_layers:\n",
        "        # Check weight and bias\n",
        "        if hasattr(layer, 'filters') and hasattr(layer, 'd_filters') and hasattr(layer, 'bias') and hasattr(layer, 'd_bias'):\n",
        "            layer.w -= self.lr * layer.d_filters\n",
        "            layer.b -= self.lr * layer.d_bias\n",
        "\n",
        "        if hasattr(layer, 'w') and hasattr(layer, 'dw') and hasattr(layer, 'b') and hasattr(layer, 'db'):\n",
        "            layer.w -= self.lr * layer.dw\n",
        "            layer.b -= self.lr * layer.db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugKSKZDqj5L4"
      },
      "source": [
        "### B. Implementing a train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_1Vli0UQuEYA"
      },
      "outputs": [],
      "source": [
        "def train(optimizer, x, y, x_tst, y_tst, N, max_epochs, thresh, log_rate):\n",
        "  # B.\n",
        "  \"\"\"\n",
        "  Trains a model using a minibatch updates applied through an optimizer class object.\n",
        "\n",
        "  Inputs:\n",
        "  -----------\n",
        "\n",
        "  optimizer : an instance of optimizer class (e.g. an instance of the GD class). This class should have\n",
        "  an lr (learning rate) and a model member variable, as well as a step() member function.\n",
        "\n",
        "  x : a numpy array holding the input features of the training data\n",
        "\n",
        "  y : a numpy array holding the labels of the training data\n",
        "\n",
        "  x_tst : a numpy array holding the input features of the test data\n",
        "\n",
        "  y_tst : a numpy array holding the labels of the test data\n",
        "\n",
        "  N : scalar (int), the minibatch size to use during training\n",
        "\n",
        "  max_epochs : scalar (int), the largest number of epochs to iterate over before terminating the function\n",
        "\n",
        "  thresh : scalar (float), loss threshold which if reached should terminate the runtime of the function\n",
        "\n",
        "  log_rate : scalar (int), the frequency with which to log the train and test loss during training (steps)\n",
        "\n",
        "\n",
        "  Returns:\n",
        "  ----------\n",
        "\n",
        "  log_epoch : a python list containing the epochs when training and test errors where logged\n",
        "\n",
        "  log_train : a python list containing the logged training errors\n",
        "\n",
        "  log_test : a python list containing the logged test errors\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  # Extract shape information from arrays\n",
        "  M = x.shape[0] # Number of points in the training data array\n",
        "  M_test = x_tst.shape[0] # Number of points in the test data array\n",
        "  n_batches = int(np.ceil(M/N)) # Number of batches per epoch\n",
        "  log_epoch = []\n",
        "  log_train = []\n",
        "  log_test = []\n",
        "  i = 0 # counter for the number of iters / epochs\n",
        "  loss = 2*thresh # loss variable initialized so enter first while loop\n",
        "\n",
        "  # EXAMPLE CODE SKETCH:\n",
        "  # While loop until either threshold or max-iters / epochs is reached\n",
        "    # Draw a random permutation over [M] where M is # training examples\n",
        "    # For loop over over the number of batches\n",
        "      # Extract next batch\n",
        "      # Calculate predictions using model\n",
        "      # Compute the loss\n",
        "      # Run backward pass through the model to compute gradients of parameters\n",
        "      # Update parameters using GD.step()\n",
        "      # if is the first batch in an iter / epoch that is supposed to be logged then log the current train and test loss (you will need to comptute test loss on a test batch)\n",
        "    # Increment the counter for the number of epochs / iters\n",
        "    # CODE HERE\n",
        "\n",
        "  while i < max_epochs and loss > thresh:\n",
        "\n",
        "      perm = np.random.permutation(M)\n",
        "      x_shuf, y_shuf = x[perm], y[perm]\n",
        "\n",
        "      for b in range(n_batches):\n",
        "          start = b * N\n",
        "          end = min(start + N, M)\n",
        "          x_batch, y_batch = x_shuf[start:end], y_shuf[start:end]\n",
        "\n",
        "          y_pred = optimizer.model.forward(x_batch)\n",
        "          loss = optimizer.model.loss(y_batch, y_pred)\n",
        "\n",
        "          optimizer.model.backward(y_batch)\n",
        "          optimizer.step()\n",
        "\n",
        "          step_idx = i * n_batches + b\n",
        "\n",
        "          if step_idx % log_rate == 0:\n",
        "              train_loss = loss\n",
        "              tst_size = min(N, M_test)\n",
        "              tst_idx = np.random.choice(M_test, size=tst_size, replace = False)\n",
        "              x_tst_mini, y_tst_mini = x_tst[tst_idx], y_tst[tst_idx]\n",
        "\n",
        "              y_tst_pred = optimizer.model.forward(x_tst_mini)\n",
        "              test_loss = optimizer.model.loss(y_tst_mini, y_tst_pred)\n",
        "              log_epoch.append(step_idx)\n",
        "              log_train.append(train_loss)\n",
        "              log_test.append(test_loss)\n",
        "\n",
        "              print(f\"[Epoch {i+1}/{max_epochs} | Batch {b+1}/{n_batches}]\",\n",
        "                      f\"train_loss={loss:.4f}\", f\"test_loss={test_loss:.4f}\")\n",
        "\n",
        "      i += 1\n",
        "\n",
        "  return log_epoch, log_train, log_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSKkocIKqIi8"
      },
      "source": [
        "### C. Training an instance of CNN_simple to fit CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "L_B3u_90sNBj"
      },
      "outputs": [],
      "source": [
        "# Set hyperparameters,\n",
        "learning_rate = 0.02\n",
        "batch_size = 512\n",
        "max_epochs = 5\n",
        "loss_thresh = 0.1\n",
        "log_rate = 1\n",
        "\n",
        "\n",
        "# normalisation of data\n",
        "mean = X_train.mean(axis=(0,2,3), keepdims=True)\n",
        "std  = X_train.std(axis=(0,2,3), keepdims=True)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test  = (X_test  - mean) / std\n",
        "\n",
        "# Create a small instance of CIFAR10\n",
        "X_train, y_train = X_train[:1500], y_train[:1500]\n",
        "X_test,  y_test  = X_test[:300],  y_test[:300]\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = CNN_simple()\n",
        "model.layers = model.param_layers\n",
        "optimizer = GD(model, lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iQhrwShuqSFx",
        "outputId": "cad0e69f-e772-4d0a-b9ca-207f87bdb09e"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19704\\512384410.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Call train function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m log_epoch, log_train, log_test = train(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19704\\3608234436.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(optimizer, x, y, x_tst, y_tst, N, max_epochs, thresh, log_rate)\u001b[0m\n\u001b[0;32m     71\u001b[0m           \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_shuf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_shuf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m           \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m           \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19704\\1422963046.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19704\\2784695240.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     56\u001b[0m                     \u001b[0mwindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mws\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                     \u001b[1;31m# Find max in KxK window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                     \u001b[0mflat_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m                     \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                     \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \"\"\"\n\u001b[1;32m-> 1195\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Call train function\n",
        "log_epoch, log_train, log_test = train(\n",
        "    optimizer,\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    N = batch_size,\n",
        "    max_epochs = max_epochs,\n",
        "    thresh = loss_thresh,\n",
        "    log_rate = log_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUOxL7N9tJyq",
        "outputId": "1d89f0a0-ef59-4c87-fc75-b4620b4fc703"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABkoElEQVR4nO3dd3gU1f4G8Heym91N3RRSIT0h9I4KSJMmIAJ25FL0qpdr6IIIyKVcJQoWUBR+YAGvBVQQqVKERJTekRZKSAIkhIT0ns35/bHJkiU92Zbk/TzPPNmdnTnz3U1IXuacmSMJIQSIiIiIGggrcxdAREREZEgMN0RERNSgMNwQERFRg8JwQ0RERA0Kww0RERE1KAw3RERE1KAw3BAREVGDwnBDREREDQrDDRERETUoDDfUYBw8eBALFixAamqqUdofP348/P39a7Xv2rVrIUkSbty4YdCaLE2fPn3Qp0+fKrdbvHgxNm/ebNRaLly4gAULFlT7M28s3yNjqezfX3V/LogMheGGGoyDBw9i4cKFRgs38+bNwy+//FKrfYcOHYpDhw7By8vLwFXVT6YKNwsXLmRYMZHK/v19/vnn+Pzzz01fFDVacnMXQGQuOTk5sLGxqfb2QUFBtT6Wm5sb3Nzcar0/kSXIzs6Gra1tjfdr1aqVEaohqhjP3FCDsGDBAsycORMAEBAQAEmSIEkSIiIiAAD+/v544oknsGnTJnTs2BEqlQoLFy4EAHz22Wfo1asX3N3dYWdnh7Zt22LJkiUoKCjQO0Z53VKSJGHixIn43//+h5YtW8LW1hbt27fHtm3b9LYrr8ujT58+aNOmDY4dO4aePXvC1tYWgYGBeO+991BUVKS3//nz5zFw4EDY2trCzc0NYWFh2L59u957rMjVq1fx0ksvISQkBLa2tmjatCmGDRuGc+fO6W0XEREBSZLwww8/YO7cufD29oajoyP69++Py5cv620rhMCSJUvg5+cHlUqFTp06YefOnZXWUfozy8rKwrp163Tfp9JdFgkJCfjXv/6FZs2aQaFQICAgAAsXLkRhYaFeOytXrkT79u1hb28PBwcHtGjRAnPmzNF93s8++ywAoG/fvrrjrF27tlo1lvbVV1+hffv2UKlUcHFxwciRI3Hx4kW9ba5fv44XXngB3t7eUCqV8PDwQL9+/XD69GndNvv27UOfPn3g6uoKGxsb+Pr64umnn0Z2dnalxy8qKsKSJUvQokULKJVKuLu7Y+zYsbh586Zum6lTp8LOzg7p6ell9n/++efh4eGh9/O8YcMGdOvWDXZ2drC3t8egQYNw6tQpvf3Gjx8Pe3t7nDt3DgMHDoSDgwP69etXbo1V/ft7sFvqxo0bkCQJS5cuxfvvvw9/f3/Y2NigT58+iIqKQkFBAd566y14e3tDrVZj5MiRSExMLHPc6rwPaqQEUQMQFxcnJk2aJACITZs2iUOHDolDhw6JtLQ0IYQQfn5+wsvLSwQGBoqvvvpK7N+/Xxw9elQIIcS0adPEypUrxW+//Sb27dsnPv74Y9GkSRPx0ksv6R1j3Lhxws/PT28dAOHv7y8eeugh8eOPP4odO3aIPn36CLlcLq5du6bb7uuvvxYARHR0tG5d7969haurqwgJCRGrVq0Se/bsEa+//roAINatW6fb7vbt28LV1VX4+vqKtWvXih07dogxY8YIf39/AUDs37+/0s8mMjJSvPHGG+Lnn38WkZGR4pdffhEjRowQNjY24tKlS7rt9u/fr3s/o0ePFtu3bxc//PCD8PX1FSEhIaKwsFC37fz58wUA8c9//lPs3LlTrF69WjRt2lR4enqK3r17V1rPoUOHhI2NjRgyZIju+3T+/HkhhBDx8fHCx8dH+Pn5if/7v/8Te/fuFf/973+FUqkU48eP17Xxww8/CABi0qRJYvfu3WLv3r1i1apVYvLkyUIIIRITE8XixYsFAPHZZ5/pjpOYmFhhXeV9j0raGDVqlNi+fbv45ptvRGBgoFCr1SIqKkq3XWhoqAgODhb/+9//RGRkpNi4caN44403dN+b6OhooVKpxIABA8TmzZtFRESE+O6778SYMWNESkpKpZ/Xa6+9JgCIiRMnit9++02sWrVKuLm5CR8fH3H37l0hhBBnzpwRAMSaNWv09k1JSRFKpVJMnz5dt+7dd98VkiSJl19+WWzbtk1s2rRJdOvWTdjZ2em+D0Jof96tra2Fv7+/CA8PF7///rvYtWtXuTVW9e+vd+/eej8X0dHRAoDw8/MTw4YNE9u2bRPffvut8PDwEM2bNxdjxowRL7/8sti5c6dYtWqVsLe3F8OGDdM7ZnXfBzVODDfUYCxdurTMH6cSfn5+QiaTicuXL1fahkajEQUFBeKbb74RMplM3Lt3T/daReHGw8NDpKen69YlJCQIKysrER4erltXUbgBII4cOaLXZqtWrcSgQYN0z2fOnCkkSSrzC3vQoEHVCjcPKiwsFPn5+SIkJERMmzZNt74k3AwZMkRv+x9//FEAEIcOHRJCaP9gqlQqMXLkSL3t/vrrLwGgynAjhBB2dnZi3LhxZdb/61//Evb29iImJkZv/QcffCAA6D6DiRMnCicnp0qP8dNPP9Xo83nwe5SSkqILYaXFxsYKpVIpXnzxRSGEEElJSQKAWLZsWYVt//zzzwKAOH36dLVqKXHx4kUBQLz++ut6648cOSIAiDlz5ujWderUSXTv3l1vu88//1wAEOfOndPVLpfLxaRJk/S2y8jIEJ6enuK5557TrRs3bpwAIL766qtq1VrZv7+Kwk379u2FRqPRrV+2bJkAIJ588km9/adOnSoA6MJSTd4HNU7slqJGo127dmjevHmZ9adOncKTTz4JV1dXyGQyWFtbY+zYsdBoNIiKiqqy3b59+8LBwUH33MPDA+7u7oiJialyX09PTzz00ENl6iy9b2RkJNq0aVNm3MKoUaOqbB8ACgsLsXjxYrRq1QoKhQJyuRwKhQJXrlwp070CAE8++WSZegDoajp06BByc3MxevRove26d+8OPz+/atVUkW3btqFv377w9vZGYWGhbhk8eDAA7WcBAA899BBSU1MxatQo/Prrr0hKSqrTcctz6NAh5OTkYPz48XrrfXx88Nhjj+H3338HALi4uCAoKAhLly7FRx99hFOnTpXpVuzQoQMUCgVee+01rFu3DtevX69WDfv37weAMjU89NBDaNmypa4GAHjppZdw8OBBvS7Er7/+Gl27dkWbNm0AALt27UJhYSHGjh2r9/mqVCr07t273C7Op59+ulq11saQIUNgZXX/z1DLli0BaAfgl1ayPjY2FkDt3gc1Lgw31GiUd6VSbGwsevbsiVu3bmH58uU4cOAAjh07hs8++wyAdtBxVVxdXcusUyqVBts3OTkZHh4eZbYrb115pk+fjnnz5mHEiBHYunUrjhw5gmPHjqF9+/bl1vhgTUqlEsD9zyI5ORmANpg9qLx1NXHnzh1s3boV1tbWekvr1q0BQBdixowZg6+++goxMTF4+umn4e7ujocffhh79uyp0/FLK3mf5f3ceHt7616XJAm///47Bg0ahCVLlqBTp05wc3PD5MmTkZGRAUA7GH3v3r1wd3dHWFgYgoKCEBQUhOXLlxukBgAYPXo0lEqlblzRhQsXcOzYMbz00ku6be7cuQMA6Nq1a5nPeMOGDWVCoq2tLRwdHSutsS5cXFz0nisUikrX5+bmAqj5+6DGh1dLUaMhSVKZdZs3b0ZWVhY2bdqkd9ah9EBQc3N1ddX9Mi8tISGhWvt/++23GDt2LBYvXqy3PikpCU5OTrWqp6LjJyQk1PpeQADQpEkTtGvXDu+++265r3t7e+sev/TSS3jppZeQlZWFP/74A/Pnz8cTTzyBqKioOp9BAu6/z/j4+DKv3b59G02aNNE99/Pzw5dffgkAiIqKwo8//ogFCxYgPz8fq1atAgD07NkTPXv2hEajwfHjx/Hpp59i6tSp8PDwwAsvvFBlDc2aNau0BmdnZwwfPhzffPMN3nnnHXz99ddQqVR6Z/hKtv/555+r9RmV92/GEtT0fVDjw3BDDcaDZxiqo+SXd8m+gPZKoDVr1hi2uDro3bs3PvjgA1y4cEGva2r9+vXV2l+SJL33BwDbt2/HrVu3EBwcXON6HnnkEahUKnz33Xd6XRYHDx5ETExMtcJNRWe2nnjiCezYsQNBQUFwdnauVj12dnYYPHgw8vPzMWLECJw/fx5+fn61+nkorVu3brCxscG3336ru/IKAG7evIl9+/bhmWeeKXe/5s2b4+2338bGjRtx8uTJMq/LZDI8/PDDaNGiBb777jucPHmywnDz2GOPAdAG1K5du+rWHzt2DBcvXsTcuXP1tn/ppZfw448/YseOHfj2228xcuRIvQA7aNAgyOVyXLt2zeDdTXX9vGvCmO+DGgaGG2ow2rZtCwBYvnw5xo0bB2tra4SGhuqNh3nQgAEDoFAoMGrUKLz55pvIzc3FypUrkZKSYqqyqzR16lR89dVXGDx4MBYtWgQPDw98//33uHTpEgDojVkozxNPPIG1a9eiRYsWaNeuHU6cOIGlS5eWORNQXc7OzpgxYwbeeecdvPLKK3j22WcRFxeHBQsWVLtbqm3btoiIiMDWrVvh5eUFBwcHhIaGYtGiRdizZw+6d++OyZMnIzQ0FLm5ubhx4wZ27NiBVatWoVmzZnj11VdhY2ODHj16wMvLCwkJCQgPD4dardaFgJJxJqtXr4aDgwNUKhUCAgLK7Qosj5OTE+bNm4c5c+Zg7NixGDVqFJKTk7Fw4UKoVCrMnz8fAHD27FlMnDgRzz77LEJCQqBQKLBv3z6cPXsWb731FgBg1apV2LdvH4YOHQpfX1/k5ubiq6++AgD079+/whpCQ0Px2muv4dNPP4WVlRUGDx6MGzduYN68efDx8cG0adP0th84cCCaNWuG119/HQkJCXpdUoD2lgiLFi3C3Llzcf36dTz++ONwdnbGnTt3cPToUdjZ2elukVBTtfn3V1vGfB/UQJh7RDORIc2ePVt4e3sLKysrvStl/Pz8xNChQ8vdZ+vWraJ9+/ZCpVKJpk2bipkzZ4qdO3eWudKmoqulwsLCyrTp5+endzVQRVdLtW7dusy+5R3n77//Fv379xcqlUq4uLiIf/7zn2LdunUCgDhz5kyln0lKSor45z//Kdzd3YWtra149NFHxYEDB8pcwVJytdRPP/2kt3/JlS1ff/21bl1RUZEIDw8XPj4+QqFQiHbt2omtW7eWabMip0+fFj169BC2trZlrrC6e/eumDx5sggICBDW1tbCxcVFdO7cWcydO1dkZmYKIYRYt26d6Nu3r/Dw8BAKhUJ4e3uL5557Tpw9e1bvOMuWLRMBAQFCJpOVeQ8PKu97JIQQX3zxhWjXrp1QKBRCrVaL4cOH6125dufOHTF+/HjRokULYWdnJ+zt7UW7du3Exx9/rLt8/tChQ2LkyJHCz89PKJVK4erqKnr37i22bNlS5Wel0WjE+++/L5o3by6sra1FkyZNxD/+8Q8RFxdX7vZz5swRAISPj4/elUilbd68WfTt21c4OjoKpVIp/Pz8xDPPPCP27t2r22bcuHHCzs6uyvpKq+jfX0VXSy1dulRv/4p+Bku+N8eOHavx+6DGSRJCCDNkKiKqo9deew0//PADkpOTdQMuiYiI3VJE9cKiRYvg7e2NwMBAZGZmYtu2bfjiiy/w9ttvM9gQET2A4YaoHrC2tsbSpUtx8+ZNFBYWIiQkBB999BGmTJli7tKIiCwOu6WIiIioQeFN/IiIiKhBYbghIiKiBoXhhoiIiBqURjeguKioCLdv34aDg4PF3lqciIiI9AkhkJGRAW9v7ypvXtrows3t27fh4+Nj7jKIiIioFuLi4qq8w3qjCzcltwKPi4sz6my3REREZDjp6enw8fGp1pQejS7clHRFOTo6MtwQERHVM9UZUsIBxURERNSgMNwQERFRg8JwQ0RERA2KxYy5CQ8Px5w5czBlyhQsW7as3G0iIiLQt2/fMusvXryIFi1aGLlCIiKqLzQaDQoKCsxdBtWQQqGo8jLv6rCIcHPs2DGsXr0a7dq1q9b2ly9f1hsM7ObmZqzSiIioHhFCICEhAampqeYuhWrBysoKAQEBUCgUdWrH7OEmMzMTo0ePxpo1a/DOO+9Uax93d3c4OTkZtzAiIqp3SoKNu7s7bG1tebPWeqTkJrvx8fHw9fWt0/fO7OEmLCwMQ4cORf/+/asdbjp27Ijc3Fy0atUKb7/9drldVURE1LhoNBpdsHF1dTV3OVQLbm5uuH37NgoLC2FtbV3rdswabtavX48TJ07g+PHj1drey8sLq1evRufOnZGXl4f//e9/6NevHyIiItCrV69y98nLy0NeXp7ueXp6ukFqJyIiy1IyxsbW1tbMlVBtlXRHaTSa+hlu4uLiMGXKFOzevRsqlapa+4SGhiI0NFT3vFu3boiLi8MHH3xQYbgJDw/HwoULDVIzERFZPnZF1V+G+t6Z7VLwEydOIDExEZ07d4ZcLodcLkdkZCQ++eQTyOVyaDSaarXzyCOP4MqVKxW+Pnv2bKSlpemWuLg4Q70FIiIiskBmCzf9+vXDuXPncPr0ad3SpUsXjB49GqdPn4ZMJqtWO6dOnYKXl1eFryuVSt1UC5xygYiIGjp/f/8Kb6liyjbMyWzdUg4ODmjTpo3eOjs7O7i6uurWz549G7du3cI333wDAFi2bBn8/f3RunVr5Ofn49tvv8XGjRuxceNGk9dPRERkCH369EGHDh0MFiaOHTsGOzs7g7RVX5n9aqnKxMfHIzY2Vvc8Pz8fM2bMwK1bt2BjY4PWrVtj+/btGDJkiBmrvC8tuwDx6Tlo4cmzQ0REZDhCCGg0GsjlVf/Z5r3fLGz6hYiICL3kunbtWkREROiev/nmm7h69SpycnJw7949HDhwwGKCTdSdDLRftBvPrToEIYS5yyEionpg/PjxiIyMxPLlyyFJEiRJwo0bNxAREQFJkrBr1y506dIFSqUSBw4cwLVr1zB8+HB4eHjA3t4eXbt2xd69e/XafLBLSZIkfPHFFxg5ciRsbW0REhKCLVu21KjO2NhYDB8+HPb29nB0dMRzzz2HO3fu6F4/c+YM+vbtCwcHBzg6OqJz5866K6FjYmIwbNgwODs7w87ODq1bt8aOHTtq/6FVg0WfualPfF1sYSUB6bmFuJuZB3eH6l0BRkRExiOEQE5B9S5QMSQba1m1rvxZvnw5oqKi0KZNGyxatAiA9szLjRs3AGj/U//BBx8gMDAQTk5OuHnzJoYMGYJ33nkHKpUK69atw7Bhw3D58mX4+vpWeJyFCxdiyZIlWLp0KT799FOMHj0aMTExcHFxqbJGIQRGjBgBOzs7REZGorCwEK+//jqef/553QmI0aNHo2PHjli5ciVkMhlOnz6tu5Q7LCwM+fn5+OOPP2BnZ4cLFy7A3t6+yuPWBcONgaisZfBxsUVMcjauJWYx3BARWYCcAg1a/WeXyY97YdEg2Cqq/hOrVquhUChga2sLT0/PMq8vWrQIAwYM0D13dXVF+/btdc/feecd/PLLL9iyZQsmTpxY4XHGjx+PUaNGAQAWL16MTz/9FEePHsXjjz9eZY179+7F2bNnER0dDR8fHwDA//73P7Ru3RrHjh1D165dERsbi5kzZ+rmeQwJCdHtHxsbi6effhpt27YFAAQGBlZ5zLqyqG6p+i7ITZtEr93NNHMlRETUEHTp0kXveVZWFt588020atUKTk5OsLe3x6VLl/TGp5an9NyNdnZ2cHBwQGJiYrVquHjxInx8fHTBBoDu+BcvXgQATJ8+Ha+88gr69++P9957D9euXdNtO3nyZLzzzjvo0aMH5s+fj7Nnz1bruHXBMzcGFORmh32XGG6IiCyFjbUMFxYNMstxDeHBq55mzpyJXbt24YMPPkBwcDBsbGzwzDPPID8/v9J2HrzbryRJKCoqqlYNQohyu9hKr1+wYAFefPFFbN++HTt37sT8+fOxfv16jBw5Eq+88goGDRqE7du3Y/fu3QgPD8eHH36ISZMmVev4tcFwY0D3z9xkmbkSIiICtH/Eq9M9ZE4KhaLaN649cOAAxo8fj5EjRwLQTj5dMj7HWFq1aoXY2FjExcXpzt5cuHABaWlpaNmypW675s2bo3nz5pg2bRpGjRqFr7/+Wlenj48PJkyYgAkTJmD27NlYs2aNUcMNu6UMKMi9ONwk8swNERFVj7+/P44cOYIbN24gKSmp0jMqwcHB2LRpE06fPo0zZ87gxRdfrPYZmNrq378/2rVrh9GjR+PkyZM4evQoxo4di969e6NLly7IycnBxIkTERERgZiYGPz11184duyYLvhMnToVu3btQnR0NE6ePIl9+/bphSJjYLgxoJIzN7dSc5CTb/rR+UREVP/MmDEDMpkMrVq1gpubW6XjZz7++GM4Ozuje/fuGDZsGAYNGoROnToZtT5JkrB582Y4OzujV69e6N+/PwIDA7FhwwYAgEwmQ3JyMsaOHYvmzZvjueeew+DBg3XzOmo0GoSFhaFly5Z4/PHHERoais8//9y4NYtGdlOW9PR0qNVqpKWlGWUqho6LdiMluwDbJz+K1t5qg7dPRETly83NRXR0NAICAqo9ITNZlsq+hzX5+80zNwbGcTdERETmxXBjYLpww3E3REREZsFwY2BB7trL9q7ycnAiIiKzYLgxsGBeMUVERGRWDDcGVtItFZ2UBU1RoxqrTUREZBEYbgysmbMtFDIr5BUW4XZqjrnLISIianQYbgxMZiUhoAnH3RAREZkLw40RlAwq5rgbIiIi02O4MQLe64aIiMh8GG6M4H644ZkbIiKyXH369MHUqVPNXYbBMdwYQUm4uc5wQ0REVTBGwBg/fjxGjBhh0DbrE4YbIwh00465ScrMR2p2vpmrISIialwYbozATimHt1o74Re7poiIqCLjx49HZGQkli9fDkmSIEkSbty4AQC4cOEChgwZAnt7e3h4eGDMmDFISkrS7fvzzz+jbdu2sLGxgaurK/r374+srCwsWLAA69atw6+//qprMyIiolr1pKSkYOzYsXB2doatrS0GDx6MK1eu6F6PiYnBsGHD4OzsDDs7O7Ru3Ro7duzQ7Tt69Gi4ubnBxsYGISEh+Prrrw32WdWE3CxHbQSC3O1xOy0X1xKz0NnPxdzlEBE1TkIABdmmP661LSBJVW62fPlyREVFoU2bNli0aBEAwM3NDfHx8ejduzdeffVVfPTRR8jJycGsWbPw3HPPYd++fYiPj8eoUaOwZMkSjBw5EhkZGThw4ACEEJgxYwYuXryI9PR0Xbhwcane36Hx48fjypUr2LJlCxwdHTFr1iwMGTIEFy5cgLW1NcLCwpCfn48//vgDdnZ2uHDhAuzttUMx5s2bhwsXLmDnzp1o0qQJrl69ipwc89zvjeHGSILc7HHgShLP3BARmVNBNrDY2/THnXMbUNhVuZlarYZCoYCtrS08PT1161euXIlOnTph8eLFunVfffUVfHx8EBUVhczMTBQWFuKpp56Cn58fAKBt27a6bW1sbJCXl6fXZlVKQs1ff/2F7t27AwC+++47+Pj4YPPmzXj22WcRGxuLp59+WneswMBA3f6xsbHo2LEjunTpAgDw9/ev9rENjd1SRhJUPO6G4YaIiGrqxIkT2L9/P+zt7XVLixYtAADXrl1D+/bt0a9fP7Rt2xbPPvss1qxZg5SUlDod8+LFi5DL5Xj44Yd161xdXREaGoqLFy8CACZPnox33nkHPXr0wPz583H27Fndtv/+97+xfv16dOjQAW+++SYOHjxYp3rqgmdujIT3uiEisgDWttqzKOY4bh0UFRVh2LBheP/998u85uXlBZlMhj179uDgwYPYvXs3Pv30U8ydOxdHjhxBQEBArY4pRPnzIQohIBV3sb3yyisYNGgQtm/fjt27dyM8PBwffvghJk2ahMGDByMmJgbbt2/H3r170a9fP4SFheGDDz6oVT11wTM3RhJUPDt47L1s5BVqzFwNEVEjJUna7iFTL9UYb1NCoVBAo9H/O9GpUyecP38e/v7+CA4O1lvs7OyK35qEHj16YOHChTh16hQUCgV++eWXCtusSqtWrVBYWIgjR47o1iUnJyMqKgotW7bUrfPx8cGECROwadMmvPHGG1izZo3uNTc3N4wfPx7ffvstli1bhtWrV9eoBkNhuDESdwcl7JVyaIoEYpPNMJiNiIjqBX9/fxw5cgQ3btxAUlISioqKEBYWhnv37mHUqFE4evQorl+/jt27d+Pll1+GRqPBkSNHsHjxYhw/fhyxsbHYtGkT7t69qwsh/v7+OHv2LC5fvoykpCQUFBRUWUdISAiGDx+OV199FX/++SfOnDmDf/zjH2jatCmGDx8OAJg6dSp27dqF6OhonDx5Evv27dMd8z//+Q9+/fVXXL16FefPn8e2bdv0QpEpMdwYiSRJHHdDRERVmjFjBmQyGVq1agU3NzfExsbC29sbf/31FzQaDQYNGoQ2bdpgypQpUKvVsLKygqOjI/744w8MGTIEzZs3x9tvv40PP/wQgwcPBgC8+uqrCA0NRZcuXeDm5oa//vqrWrV8/fXX6Ny5M5544gl069YNQgjs2LED1tbWAACNRoOwsDC0bNkSjz/+OEJDQ/H5558D0J4tmj17Ntq1a4devXpBJpNh/fr1xvnQqiCJijrZGqj09HSo1WqkpaXB0dHRqMeavuE0Np26hZmDQhHWN9ioxyIiauxyc3MRHR2NgIAAqFQqc5dDtVDZ97Amf7955saISsbdXOXs4ERERCbDcGNEnECTiIjI9BhujCjYvXjMTWJmhZfYERERkWEx3BiRr4sdZFYSsvI1uJOeZ+5yiIiIGgWGGyNSyK3g56K9kRO7poiITINnyusvQ33vGG6MLJDjboiITKLkcuXsbN5brL7Kz88HAMhksjq1w+kXjCzI3Q57L2rH3RARkfHIZDI4OTkhMTERAGBra6ubNoAsX1FREe7evQtbW1vI5XWLJww3RsY5poiITKdkFuySgEP1i5WVFXx9fescShlujIyXgxMRmY4kSfDy8oK7u3u1phwgy6JQKGBlVfcRMww3RhZcHG7i03KRmVcIeyU/ciIiY5PJZHUet0H1FwcUG5na1hpN7JUAgOs8e0NERGR0DDcmwAk0iYiITIfhxgRK5pi6lshBxURERMbGcGMCHFRMRERkOgw3JsBuKSIiItNhuDGBkjM3N5KyUagpMnM1REREDRvDjQk0dbKBUm6FfE0RbqbkmLscIiKiBo3hxgSsrCTOMUVERGQiDDcmwnE3REREpsFwYyIl426ucgJNIiIio2K4MZFgd06gSUREZAoMNyZS+syNEMLM1RARETVcDDcmEtDEDpIEpOUU4F5WvrnLISIiarAYbkzERiFDUycbAOyaIiIiMiaGGxPiNAxERETGx3BjQrpwwyumiIiIjIbhxoSC3HmvGyIiImNjuDGh+91SHHNDRERkLAw3JlQSbuJSspFboDFzNURERA0Tw40JNbFXQG1jDSGA6CSevSEiIjIGhhsTkiSJc0wREREZmcWEm/DwcEiShKlTp1a6XWRkJDp37gyVSoXAwECsWrXKNAUayP0rpnjmhoiIyBgsItwcO3YMq1evRrt27SrdLjo6GkOGDEHPnj1x6tQpzJkzB5MnT8bGjRtNVGndBbnzXjdERETGZPZwk5mZidGjR2PNmjVwdnaudNtVq1bB19cXy5YtQ8uWLfHKK6/g5ZdfxgcffGCiauuON/IjIiIyLrOHm7CwMAwdOhT9+/evcttDhw5h4MCBeusGDRqE48ePo6CgwFglGlTJmJvrd7NQVMQJNImIiAxNbs6Dr1+/HidOnMDx48ertX1CQgI8PDz01nl4eKCwsBBJSUnw8vIqs09eXh7y8vJ0z9PT0+tWdB35uNjCWiYhp0CD+PRc3XxTREREZBhmO3MTFxeHKVOm4LvvvoNKpar2fpIk6T0XQpS7vkR4eDjUarVu8fHxqX3RBmAts4Kfa/EVU5yGgYiIyODMFm5OnDiBxMREdO7cGXK5HHK5HJGRkfjkk08gl8uh0ZS9yZ2npycSEhL01iUmJkIul8PV1bXc48yePRtpaWm6JS4uzijvpyZ4OTgREZHxmK1bql+/fjh37pzeupdeegktWrTArFmzIJPJyuzTrVs3bN26VW/d7t270aVLF1hbW5d7HKVSCaVSabjCDUA7qPgOrvLMDRERkcGZLdw4ODigTZs2euvs7Ozg6uqqWz979mzcunUL33zzDQBgwoQJWLFiBaZPn45XX30Vhw4dwpdffokffvjB5PXXRTAvByciIjIas18tVZn4+HjExsbqngcEBGDHjh2IiIhAhw4d8N///heffPIJnn76aTNWWXOcQJOIiMh4JFEyIreRSE9Ph1qtRlpaGhwdHc1SQ0ZuAdou2A0AODN/INQ25XepERERkVZN/n5b9JmbhspBZQ0PR+04oOvsmiIiIjIohhszYdcUERGRcTDcmAmnYSAiIjIOhhsz0d3rhpeDExERGRTDjZlwdnAiIiLjYLgxk5JuqZjkbBRoisxcDRERUcPBcGMmXmoVbBUyFBYJxCRnm7scIiKiBoPhxkwkSeKgYiIiIiNguDEjTqBJRERkeAw3ZqQ7c5PIe90QEREZCsONGfGKKSIiIsNjuDGj0mNuGtkUX0REREbDcGNGfq62sJKAjNxC3M3MM3c5REREDQLDjRmprGXwcbEFwHE3REREhsJwY2a8HJyIiMiwGG7MrORy8KucY4qIiMggGG7MLJhXTBERERkUw42ZlXRLXb/LMTdERESGwHBjZiXh5lZqDrLzC81cDRERUf3HcGNmznYKuNgpAPDsDRERkSEw3FgAzjFFRERkOAw3FuD+5eA8c0NERFRXDDcWgPe6ISIiMhyGGwsQ5F7cLcV73RAREdUZw40FKDlzE52UBU0RJ9AkIiKqC4YbC9DM2RYKuRXyCotwKyXH3OUQERHVaww3FkBmJSGwCa+YIiIiMgSGGwvBQcVERESGwXBjIXivGyIiIsNguLEQQSUTaCbyXjdERER1wXBjIdgtRUREZBgMNxYioHhAcXJWPlKy8s1cDRERUf3FcGMh7JRyeKtVAIDrSTx7Q0REVFsMNxaE426IiIjqjuHGgpSMu7nKcTdERES1xnBjQe6fuWG4ISIiqi2GGwvCe90QERHVHcONBQku7paKvZeNvEKNmashIiKqnxhuLIibgxIOSjmKBBCTnG3ucoiIiOolhhsLIkkSAjnuhoiIqE4YbiwMx90QERHVDcONhbk/DQPvdUNERFQbDDcWhnNMERER1Q3DjYUJdi/ulkrMhBDCzNUQERHVPww3FsbP1Q5yKwlZ+RokpOeauxwiIqJ6h+HGwljLrODraguAc0wRERHVBsONBeK4GyIiotpjuLFADDdERES1x3BjgXivGyIiotpjuLFA92cH55gbIiKimmK4sUBBTbThJiE9F5l5hWauhoiIqH5huLFAaltrNLFXAgCus2uKiIioRhhuLBTH3RAREdUOw42F4rgbIiKi2mG4sVDBxZeDX03kmRsiIqKaYLixULozN+yWIiIiqhGGGwtVMubmRnIWCjVFZq6GiIio/mC4sVDeahuorK1QoBGIS8kxdzlERET1BsONhbKykhDYpGRQMbumiIiIqovhxoJx3A0REVHNmTXcrFy5Eu3atYOjoyMcHR3RrVs37Ny5s8LtIyIiIElSmeXSpUsmrNp0eK8bIiKimpOb8+DNmjXDe++9h+DgYADAunXrMHz4cJw6dQqtW7eucL/Lly/D0dFR99zNzc3otZrD/dnBea8bIiKi6jJruBk2bJje83fffRcrV67E4cOHKw037u7ucHJyMnJ15hdU6l43QghIkmTmioiIiCyfxYy50Wg0WL9+PbKystCtW7dKt+3YsSO8vLzQr18/7N+/v9Jt8/LykJ6errfUF4FudpAkIC2nAMlZ+eYuh4iIqF4we7g5d+4c7O3toVQqMWHCBPzyyy9o1apVudt6eXlh9erV2LhxIzZt2oTQ0FD069cPf/zxR4Xth4eHQ61W6xYfHx9jvRWDU1nL0MzZBgCvmCIiIqouSQghzFlAfn4+YmNjkZqaio0bN+KLL75AZGRkhQHnQcOGDYMkSdiyZUu5r+fl5SEvL0/3PD09HT4+PkhLS9Mbt2Opxn99FBGX72LxyLZ48WFfc5dDRERkFunp6VCr1dX6+232MzcKhQLBwcHo0qULwsPD0b59eyxfvrza+z/yyCO4cuVKha8rlUrd1VglS31yf1Axz9wQERFVh9nDzYOEEHpnWqpy6tQpeHl5GbEi82K4ISIiqhmzXi01Z84cDB48GD4+PsjIyMD69esRERGB3377DQAwe/Zs3Lp1C9988w0AYNmyZfD390fr1q2Rn5+Pb7/9Fhs3bsTGjRvN+TaMive6ISIiqhmzhps7d+5gzJgxiI+Ph1qtRrt27fDbb79hwIABAID4+HjExsbqts/Pz8eMGTNw69Yt2NjYoHXr1ti+fTuGDBlirrdgdCV3Kb6ZkoPcAg1U1jIzV0RERGTZzD6g2NRqMiDJEggh0GHRHqTlFGDnlJ5o6WX5NRMRERlavRpQTJWTJIldU0RERDXAcFMP6AYVJ3IaBiIioqow3NQDwcXjbq7yzA0REVGVGG7qgftnbhhuiIiIqsJwUw+UXDF1PSkTRUWNavw3ERFRjTHc1AM+zjawlknILSjC7bQcc5dDRERk0Rhu6gG5zAr+riVXTHFQMRERUWUYbuoJjrshIiKqHoabeiLInfe6ISIiqg6Gm3qCE2gSERFVD8NNPXE/3HDMDRERUWUYbuqJksvB72bkIS2nwMzVEBERWS6Gm3rCXimHp6MKALumiIiIKsNwU4/oBhXziikiIqIKMdzUIxx3Q0REVDWGm3qEV0wRERFVrVbhZt26ddi+fbvu+ZtvvgknJyd0794dMTExBiuO9DHcEBERVa1W4Wbx4sWwsbEBABw6dAgrVqzAkiVL0KRJE0ybNs2gBdJ9JWNuYpOzUaApMnM1RERElklem53i4uIQHBwMANi8eTOeeeYZvPbaa+jRowf69OljyPqoFE9HFWwVMmTnaxCTnI3g4svDiYiI6L5anbmxt7dHcnIyAGD37t3o378/AEClUiEnh7NWG4skSeyaIiIiqkKtztwMGDAAr7zyCjp27IioqCgMHToUAHD+/Hn4+/sbsj56QJCbHc7dSmO4ISIiqkCtztx89tln6NatG+7evYuNGzfC1dUVAHDixAmMGjXKoAWSvpKuqKu81w0REVG5anXmxsnJCStWrCizfuHChXUuiCrHe90QERFVrlZnbn777Tf8+eefuuefffYZOnTogBdffBEpKSkGK47KKplj6npiJoQQZq6GiIjI8tQq3MycORPp6ekAgHPnzuGNN97AkCFDcP36dUyfPt2gBZI+P1dbWElARl4h7mbkmbscIiIii1Orbqno6Gi0atUKALBx40Y88cQTWLx4MU6ePIkhQ4YYtEDSp5TL4OtiixvJ2bh6NxPuxZNpEhERkVatztwoFApkZ2cDAPbu3YuBAwcCAFxcXHRndMh4OO6GiIioYrU6c/Poo49i+vTp6NGjB44ePYoNGzYAAKKiotCsWTODFkhlBbnb4/dLiZwdnIiIqBy1OnOzYsUKyOVy/Pzzz1i5ciWaNm0KANi5cycef/xxgxZIZQW5aadh4L1uiIiIyqrVmRtfX19s27atzPqPP/64zgVR1Uq6pa6zW4qIiKiMWoUbANBoNNi8eTMuXrwISZLQsmVLDB8+HDKZzJD1UTlKws2t1Bxk5RXCTlnrbyMREVGDU6u/ilevXsWQIUNw69YthIaGQgiBqKgo+Pj4YPv27QgKCjJ0nVSKs50CrnYKJGflIzopC22aqs1dEhERkcWo1ZibyZMnIygoCHFxcTh58iROnTqF2NhYBAQEYPLkyYaukcrBCTSJiIjKV6szN5GRkTh8+DBcXFx061xdXfHee++hR48eBiuOKhbkboejN+7xiikiIqIH1OrMjVKpREZGRpn1mZmZUCgUdS6KqsZ73RAREZWvVuHmiSeewGuvvYYjR45ACAEhBA4fPowJEybgySefNHSN9YuJ5ntitxQREVH5ahVuPvnkEwQFBaFbt25QqVRQqVTo3r07goODsWzZMgOXWE8U5AB7FwA73zTJ4XSXgydlQVPECTSJiIhK1GrMjZOTE3799VdcvXoVFy9ehBACrVq1QnBwsKHrqz/izwB/Ft/np/VTgF83ox6uqbMNFHIr5BcW4VZKDnxdbY16PCIiovqi2uGmqtm+IyIidI8/+uijWhdUb/k+AnQaC5z8Btg6BZhwAJArjXY4mZWEwCZ2uJSQgWt3MxluiIiIilU73Jw6dapa20mSVOti6r0Bi4DLO4Gky8Cfy4A+s4x6uCA3e1246dvC3ajHIiIiqi+qHW72799vzDoaBhtn4PH3gI3/BA58ALR5CmgSYrTDBblrx91c5eXgREREOrUaUEyVaPM0EDwA0ORru6eKiox2KE6gSUREVBbDjaFJEjD0Q8DaFoj5Czj9rdEOxXvdEBERlcVwYwzOfkDfOdrHu98GMhONcpjA4jM397LycS8r3yjHICIiqm8Ybozl4X8Dnu2A3DTgt7eMcghbhRxNnWwAANfZNUVERASA4cZ4ZHLgyU8AyQr4eyNwZY9RDhPIcTdERER6GG6Mybsj8Mjr2sfbpgP5hh8bw3E3RERE+hhujK3PbEDtC6TFAvsXG7z5ksvBOTs4ERGRFsONsSnttVdPAcDhz4Hbpw3aPC8HJyIi0sdwYwrNB2rnmxJF2nvfaAoN1nRw8Zmb2HvZyC3QGKxdIiKi+orhxlQefw9QqYH408DR/zNYs272Sjio5CgSQExytsHaJSIiqq8YbkzFwQMY8F/t433vACkxBmlWkqRSg4rZNUVERMRwY0odxwC+3YGCbGD7G4AQBmlWF244qJiIiIjhxqSsrIBhywCZAri6Bzi/ySDNBrlzUDEREVEJhhtTcwsFer6hfbzzLSAnpc5N8l43RERE9zHcmMOj04AmzYGsRGDP/Do3V3rMjTBQVxcREVF9xXBjDnIlMGy59vHJdcCNv+rUnJ+rLeRWErLzNUhIzzVAgURERPUXw425+HUHOo3TPt46BSjMq3VT1jIr+LraAgCuJbJrioiIGjeGG3MasBCwcweSrwB/flynpng5OBERkRbDjTnZOAOD39c+PvAhcPdyrZsquVPxVV4OTkREjZxZw83KlSvRrl07ODo6wtHREd26dcPOnTsr3ScyMhKdO3eGSqVCYGAgVq1aZaJqjaT1SCBkEKDJB7ZOBYqKatUMz9wQERFpmTXcNGvWDO+99x6OHz+O48eP47HHHsPw4cNx/vz5crePjo7GkCFD0LNnT5w6dQpz5szB5MmTsXHjRhNXbkCSBAz9ALC2BWIPAqe+qVUznECTiIhISxIWdu2wi4sLli5din/+859lXps1axa2bNmCixcv6tZNmDABZ86cwaFDh6rVfnp6OtRqNdLS0uDo6Giwuuvs0GfArjmAUg1MPKadrqEG0nIK0H7hbgDAuQUD4aCyNkaVREREZlGTv98WM+ZGo9Fg/fr1yMrKQrdu3crd5tChQxg4cKDeukGDBuH48eMoKCgod5+8vDykp6frLRbpoX8BXh2AvDTgt7dqvLvaxhreahUAIHznJd7vhoiIGi2zh5tz587B3t4eSqUSEyZMwC+//IJWrVqVu21CQgI8PPTPaHh4eKCwsBBJSUnl7hMeHg61Wq1bfHx8DP4eDEImB578BJBk2mkZonbVuIm3n2gFKwn4/kgs/rvtIgMOERE1SmYPN6GhoTh9+jQOHz6Mf//73xg3bhwuXLhQ4faSJOk9L/kD/uD6ErNnz0ZaWppuiYuLM1zxhubVHnjk39rH298A8mo2fmZIWy8seaY9AOCrv6Lx4e4oQ1dIRERk8cwebhQKBYKDg9GlSxeEh4ejffv2WL58ebnbenp6IiEhQW9dYmIi5HI5XF1dy91HqVTqrsYqWSxa3zmA2hdIiwP2L67x7s90bob/jmgDAFix/ypW7Lti6AqJiIgsmtnDzYOEEMjLK/9uvd26dcOePXv01u3evRtdunSBtXUDGUCrsAOe+Ej7+MhK4PapGjcx5hE/vD20JQDgg91R+OLAdUNWSEREZNHMGm7mzJmDAwcO4MaNGzh37hzmzp2LiIgIjB49GoC2S2ns2LG67SdMmICYmBhMnz4dFy9exFdffYUvv/wSM2bMMNdbMI6QAUCbZwBRBGyZDGgKa9zEKz0D8caA5gCAd7ZfxLeHYwxdJRERkUUya7i5c+cOxowZg9DQUPTr1w9HjhzBb7/9hgEDBgAA4uPjERsbq9s+ICAAO3bsQEREBDp06ID//ve/+OSTT/D000+b6y0Yz+PhgEoNJJzVnsGphYmPBePffYIAAG9v/hsbT9w0ZIVEREQWyeLuc2NsFnufm/Kc/AbYMkl7g7/XDwHO/jVuQgiBhVsvYO3BG7CSgE9HdcLQdl6Gr5WIiMiI6uV9bqgcHccAfo8CBdnaq6dqkUMlScL8Ya0w6iEfFAlgyvpT2HvhjhGKJSIisgwMN5ZMkoBhywCZAri6F/i7dtNMSJKEd0a0xciOTVFYJPD6dydx4Mpdw9ZKRERkIRhuLF2TEKDXTO3j394Csu/VqhmZlYSlz7TD4DaeyNcU4dVvjuPI9WQDFkpERGQZGG7qgx5TgSahQNZdYM9/at2MXGaF5S90xGMt3JFbUISX1x7DqdgUw9VJRERkARhu6gO5AhhWfGPDU/8DbvxZ66YUcit8ProTegS7Iitfg3FfHcX522kGKpSIiMj8GG7qC79uQOeXtI+3TgUKcmvdlMpahjVju6CrvzPScwsx5sujuHInwzB1EhERmRnDTX3SfwFg7wEkXwH+/KhOTdkq5PhqfFe0b6bGvax8vPjFEUQnZRmmTiIiIjNiuKlPbJyAwe9rHx/4CEi8VKfmHFTWWPfyQ2jh6YC7GXkYveYwbqZk171OIiIiM2K4qW9ajQCaPw4UFQBbpwBFRXVqzslWgW9feRhBbna4nZaLF9ccQUJa7bu8iIiIzI3hpr6RJGDIB4C1HRB3GDi5ts5NNrFX4rtXHoGviy1i72Vj9BeHkZRZ/uSlRERElo7hpj5y8gH6zdM+3rMAyEioc5OeahW+e+VheKtVuHY3C2O+PIrU7Pw6t0tERGRqDDf11UOvAd4dgbw0YOcsgzTp42KL7159BG4OSlyMT8e4r44iI7fAIG0TERGZCsNNfWUl0977RpIBFzYDl3capNmAJnb47pWH4WxrjTM30/Dy2mPIzi80SNtERESmwHBTn3m1B7qFaR9vnwHkZRqk2eYeDvjfPx+Gg0qOYzdS8Oo3x5FboDFI20RERMbGcFPf9XkLcPID0m8C+981WLNtmqqx7uWHYKeQ4a+ryXj9u5PIL6zblVlERESmwHBT3ynsgCeKb+h3ZBVw64TBmu7k64wvx3eFytoK+y4lYsr6UyjUMOAQEZFlY7hpCIL7A22fBUQRsGUKoDHcIOBHAl2xekwXKGRW2Pl3Amb+fBZFRcJg7RMRERkaw01DMSgcsHEG7pwDfl9k0KZ7NXfDZ6M7QW4l4ZdTtzB38zkIwYBDRESWieGmobB3A4Z9on188BPg8m8GbX5AKw98/HwHWEnAD0fjsHDrBQYcIiKySAw3DUmrJ4GHJ2gf//IvIDXOoM0Pa++NJc+0BwCsPXgDS3ZdZsAhIiKLw3DT0Az4L+DdCchNBX5+CSg07F2Gn+ncDO+MaAMAWBlxDSv2XTVo+0RERHXFcNPQyBXAs18DKjVw8xjw+0KDH+Ifj/jh7aEtAQAf7onCFweuG/wYREREtcVw0xA5+wPDP9c+PrQCuLTD4Id4pWcg3hjQHADwzvaL+N/hGIMfg4iIqDYYbhqqlk8Aj7yufbx5ApBi+PAx8bFgvN4nCAAwb/Pf+PnETYMfg4iIqKYYbhqy/guBpp2B3DSjjL+RJAkzB4XipR7+AIA3fz6DrWduG/QYRERENcVw05DJFcAzxeNvbp0A9i4w+CEkScJ/nmiFUQ/5okgAUzecxsFrSQY/DhERUXUx3DR0zn7AiFXax4c/Ay5uM/ghJEnCuyPa4Mn23tAUCcz86Swy8ziTOBERmQfDTWPQYgjQbaL28a+vAyk3DH4IKysJ4U+1RTNnG9xKzUH4josGPwYREVF1MNw0Fv0XAM26asff/GT48TcAYKeUY8kz7QAA3x2JxZ9X2D1FRESmx3DTWMisgWe+AlROwO2TwJ7/GOUw3YOaYGw3PwDArI1nkZFruEk8iYiIqoPhpjFx8gVGFo+/ObISuLDFKIeZ9XgL+Lhou6cW77hklGMQERFVhOGmsQkdDHSfpH3860TgXrTBD2GnlGPJ09o5qH44GosDV+4a/BhEREQVYbhpjPrNB5o9BOSlAT+NBwrzDH6IbkGuGFfSPfUzu6eIiMh0GG4aI5m1dv4pG2cg/jSw+22jHGbW4BbwdbHF7bRcvLudV08REZFpMNw0VupmwMj/0z4+uho4v9ngh7BVyLG0+Oqp9cfiEBnF7ikiIjI+hpvGrPkgoMcU7eMtk4B7hp/d++FAV4zv7g8AeGvjWaSze4qIiIyM4aaxe2we4PMIkJcO/DgOKMg1+CHefDwUfq62iE/Lxbvb2D1FRETGxXDT2JXc/8bGBUg4C+yea/BDaLun2kOSgA3H47D/cqLBj0FERFSC4YYAdVPgqdXax8e+AP7eZPBDPBTggpe6BwAAZm88h7Qcdk8REZFxMNyQVsgA4NFp2sdbJgPJ1wx+iJmDQhHQxA4J6bl4Z9sFg7dPREQEMNxQaX3fBny7AfkZwE+GH39jo5Bh6TPtIEnATyduYv8ldk8REZHhMdzQfTK5dvyNrSuQcA7YNdvgh+ji74KXe2i7p97adBZp2eyeIiIiw2K4IX2O3vfH3xz/Cjj3s8EPMWNgKAKb2OFOeh4WsXuKiIgMjOGGygruD/R8Q/t46xQg6apBm7dRyLD0WW331MaTN7Hv0h2Dtk9ERI0bww2Vr88cwO9RID9TO/9UQY5Bm+/s54JXHi3untp4jt1TRERkMAw3VD6ZHHj6C8C2CXDnHPDbWwY/xBsDQxHoZofEjDws3Hbe4O0TEVHjxHBDFXP0Ap5eA0ACTqwFzv5k0OZV1jIsfaY9rCRg08lb2HuB3VNERFR3DDdUuaDHgF4ztY+3TQWSrhi0+c5+zni1ZyAAYPYv55CanW/Q9omIqPFhuKGq9XkL8O+pHX/z4ziDj7+ZNqA5gtzscDcjDwu38uopIiKqG4YbqpqVTDv+xs4NSDwP7HzToM2rrGX44Flt99Qvp25hD7uniIioDhhuqHocPLUBBxJw8hvgzAaDNt/R1xmv9tJ2T8355RxSstg9RUREtcNwQ9UX2AfoPUv7eNs04G6UQZuf1r85gt3tcTcjDwu28uopIiKqHYYbqpnebwIBvYCCLO38U/nZBmu6dPfUr6dvY9f5BIO1TUREjQfDDdWMlQx46gvAzh1IvADsnGnQ5jv4OOFfvYMAAHN/+ZvdU0REVGMMN1RzDh7a8TeSFXDqW+D0DwZtfmr/EIS42yMpMw/zt7B7ioiIaobhhmonsDfQu/iuxdunA4mXDNa0Uq7tnpJZSdhy5jZ++zveYG0TEVHDx3BDtddrhnaQcUF28fibLIM13d7HCRN6a6+eenvz37jH7ikiIqomhhuqPSsZ8NQawN4DuHsJ2GHY8TeT+4Ug1MMBSZn5+M+vfxu0bSIiargYbqhu7N2Bp7/Ujr85/R1w6juDNV26e2rb2XjsPMfuKSIiqhrDDdVdQE+gzxzt4+1vAJe2G6zpts3U+Hfx1VNvb/4byZl5BmubiIgaJrOGm/DwcHTt2hUODg5wd3fHiBEjcPny5Ur3iYiIgCRJZZZLlww3oJVqoed07SSbhTnA+heB718AUmIM0vSkfsFo4emA5Kx8/OdXXj1FRESVM2u4iYyMRFhYGA4fPow9e/agsLAQAwcORFZW1QNTL1++jPj4eN0SEhJigoqpQlYy4PnvgEenAVZyIGon8NnDwIEPgcK6DQYu3T21/Vw8tp9l9xQREVVMEkIIcxdR4u7du3B3d0dkZCR69epV7jYRERHo27cvUlJS4OTkVONjpKenQ61WIy0tDY6OjnWsmMqVeAnYMQO4cUD7vElzYOiH2jsb18FHuy/jk31X4WKnwO5pvdDEXmmAYomIqD6oyd9vixpzk5aWBgBwcXGpctuOHTvCy8sL/fr1w/79+yvcLi8vD+np6XoLGZl7C2DcVmDkau1M4klRwLphwKbXgMzEWjc78bEQtPB0wL0sXj1FREQVs5hwI4TA9OnT8eijj6JNmzYVbufl5YXVq1dj48aN2LRpE0JDQ9GvXz/88ccf5W4fHh4OtVqtW3x8fIz1Fqg0SQLaPw9MPAZ0fQWABJzdAHzaBTi6BijS1LhJhdwKHzzbHnIrCTvOJWDb2duGr5uIiOo9i+mWCgsLw/bt2/Hnn3+iWbNmNdp32LBhkCQJW7ZsKfNaXl4e8vLuX2GTnp4OHx8fdkuZ2q0TwLbpQPxp7XPvjsDQj4CmnWrc1Md7orD89ytwtrXG7mm94ebA7ikiooau3nVLTZo0CVu2bMH+/ftrHGwA4JFHHsGVK1fKfU2pVMLR0VFvITNo2hl4dR8w5ANA6QjcPgWseQzYPgPISa1RU2F9g9HSyxEp2QWYt/lvWEg+JyIiC2HWcCOEwMSJE7Fp0ybs27cPAQEBtWrn1KlT8PLyMnB1ZHBWMuChV4GJx4G2zwEQwLE1wIquwNkfgWqGFG33VDvIrST8dj4BW3n1FBERlWLWcBMWFoZvv/0W33//PRwcHJCQkICEhATk5OTotpk9ezbGjh2re75s2TJs3rwZV65cwfnz5zF79mxs3LgREydONMdboNpw8ACeXqMddNykOZCVCGx6VTvo+G5UtZpo7a3GxMeCAQDzf/0bdzN4cz8iItIya7hZuXIl0tLS0KdPH3h5eemWDRs26LaJj49HbGys7nl+fj5mzJiBdu3aoWfPnvjzzz+xfft2PPXUU+Z4C1QXAb2ACX8Bj80D5CrtpeMruwO/LwLys6vcPaxvMFoVd0+9vfkcu6eIiAiABQ0oNhXe58ZCpdwAds4Con7TPnfyBQYvBUIfr3S3i/HpeHLFnyjQCCx/oQOGd2hq/FqJiMjk6t2AYiI4+wOj1gMvfA+ofYDUWOCH54H1o4HUuAp3a+nliEmPae9OPX/LeSRm5JqoYCIislQMN2Q5JAloMRQIOwL0mKqdxuHSNuCzh4A/lwGagnJ3+3efILRp6ojU7ALM/YVXTxERNXYMN2R5FHbAgIXAhD8Bvx5AQTawdz6w6lHgxl9lNreWaW/uZy2TsOfCHbzx4xnsv5SIvMKa3yiQiIjqP465IcsmBHBmPbD7bSA7Sbuu/ShgwH8Beze9TVdGXMP7v92fHd5OIUOfFu4Y1NoTfULd4KiyNmXlRERkQDX5+81wQ/VD9j1g33+B418DEIBKDfSbD3R+CbC6fwLy4NUk7Pg7HrvP30FiqcvDrWUSugc1wcDWHhjQygPuDiozvAkiIqothptKMNzUczePA9umAQlntc+bdtZO4+DdQW+zoiKBMzdTsfvCHew6n4Drd7N0r0kS0NHHCYNae2Jga08ENLEz4RsgIqLaYLipBMNNA1CkAY59Aex7B8hLByQroOurwGNztWd0ynE1MRO7zidg94U7OBOXqvdacw97DGzliUGtPdGmqSMkSTLBmyAioppguKkEw00DkpEA7JoL/P2z9rm9BzBoMdBqOCCreHxNfFoO9l64g13n7+Dw9WQUFt3/J+CtVmFga08MbOWBhwJcIJdxzD0RkSVguKkEw00DdD0C2P4GkHxV+1ySAU4+gHMA4BJYagnQ3k/H2ka3a1p2AfZdvoPd5+8g4vJd5BTcv8LKydYajxUPSO4V4gYbhcy074uIiHQYbirBcNNAFeYBBz8B/voUyEurfFvHptqw4+yvF35yHXzxZ2wedp1PwN6Ld5CSff++OiprK/QKccPA1p7o18IdznYK474fIiLSw3BTCYabBk4IIPMOcO968RKt/7iq4GPnBrgEosg5ALckTxxLd8KOWzY4lu6MNNgDAGRWEh7yd8Gg1h4Y0NoTTZ1sKm+TiIjqjOGmEgw3jZgQQE5KqbDzQAAquY9OBXJlDoiFBy7lu+GG8ECs8MCNIg/YeIbgodYtMKitF0Lc7TkgmYjICBhuKsFwQxXKTdMGnZTossEnI77SXbOEErHCA4nW3pA1CYZjs5bwa94O6matAVsX7fXnRERUaww3lWC4oVrJz9bOXF4SeooDkCbpOqT0m7BCUYW7ZssckKcOhI1nKFSeoYBrMNAkRDvWx5pdWkRE1cFwUwmGGzK4wnwgNRY5d64g+vJZZNy+BOvUa3AvuIVmUsVdXQISJLUP0CRYG3hcQwDXIG3wcWymd+dlIqLGjuGmEgw3ZCrJmXk4cfUWrl06h3ux56FIi0ag1W0ESgkIlG5DLWVXvLNcBbgE3Q87pcOPrYvp3gQRkYVguKkEww2ZS2p2Po5G38OR6Hs4fC0JCQm3EIDbCLBKQJAUjwApHqHyBDRDAuSisOKGbF3vh53SZ31cAgC50nRviIjIhBhuKsFwQ5YiLacAx28Uh53ryfj7VhqKBCCDBk2lJARK8ehom4QuDskItkqAa14c5Jm3K25QsgKcfAEnP+3dmh08AHtPwMETsHcvfuwBKB05wJmI6h2Gm0ow3JClysgtwPGYFBy5rg07526lQVOk/88z0FFgsHc2ujulopUyEU7ZMZCSrwDJ17TzbFWH3EYbdhw8i0NQ6fBTvM7eA7BrAljxrsxEZBkYbirBcEP1RVZeIU7EpOBIdDIOX7+HszdTUaDR/+fq7qDEw4GueNjfGT28BPxxC1LaLSAzAchM1M6/lXmn+Gti1TcxLE2SaW9q6OBxP/CUDj+lH1urDPzuiYj0MdxUguGG6qucfA1OxqbgyPVkHI6+h9OxqcjX6F+C3sReiTZNHeGltoG3WgUvJxt4qVXFiw1skKcNOyVLxh1tEMooWVf8OOsugBr8alCp73d72TgDVnJtOLKSa6/6kmTas0C6dTJtN1qt18lKtfvAOrlKe5m9gxe734gaEIabSjDcUEORW6DBqdhUHIlOxpHr93AyNgV5hRXfbwcAnG2t4akLPtrAUxJ8vJ1U8HBUQWUtAzSF2js2P3jmJzPh/rqScKTJM9E7riGlo/ZKsyahgFvz4q+h2jFJMrm5qyOiGmK4qQTDDTVUeYUanIlLw/W7mbidlouEtBzEp+Xidqr2a3a+pupGALjaKcoNPp6OKng72cDDUQWFvPgePEIAuamlzvzcAXJSAaEBijRAUWHx46Jy1mkAUWTgdRogLxNIjdG+Xh6Z4v6NFEsCT5Pm2ue8qSKRxWK4qQTDDTVGQgik5xYiPi0H8am5iE/LRXxaDm6n5iIhXbvudloOcgsqP/MDaHt6mtgr9bq7vIq7wLzVKvi62MLd0cxjcArztIOsk6K0y93LQNJlIOkqUJhTwU6S9mqzkrDjFqoNP01CeG8hIgvAcFMJhhui8gkhkJZTgNupxcGn5OxPcfBJSMvF7bRc5FfR9QUAHo5KtG/mhPY+TmjfzAltm6mhtrE2wbuoQlERkBYL3I0qDjtR9x/npFS8n53bA91bxV8dvTmuh8hEGG4qwXBDVHtCCNzLyi8+81Pq7E9xGIpPy8GtlBwUlfNbJdDNDh2aOaFdMzXa+zihpZejdnyPJRACyErShpy7l0ud7YkC0m9VvJ/Cobh7q/n9wNOkufZGiwo77U0VGX6IDILhphIMN0TGlZ1fiPO303EmLhVnbqbhTFwqYu+VnWrCWiahpZejNuw0c0IHHycEutlDZmVhYSAvA0i6oh947l7WTqAqqhjHZCXXhhyFg/ar0r4az0vWFX9VOug/572HqJFiuKkEww2R6d3LyseZm6k4G5eGMzdTcSYuFclZ+WW2s1fK0bapGu181OhQ3K3lpVZBssSzH4X52oCTdLm4a6u4eyv5GpCfabzjWtuWCjv2pQJSqWBk46y9CaOd2/2vtk2KL9PnhKxUPzHcVILhhsj8hBC4mZKjDTw303A6LhXnbqYhp6DsmRA3ByXaF5/dae+j7dZyslWYoeoa0BQCBVlAfpb26q38kqW85xnar/lZ99fnZZZ9XtVZouqQrLRdZnZu97/qwk85z22czdutJgRQkA3kpgO5adolr+RxavHXcl4TRYDaB3D2L178tF/VPoDMAsZ+Ua0w3FSC4YbIMhVqinD1bibOxqXhdPHZnUsJGWWmoAAAf1db3WDl9j5OaO1tQeN3jEEI7RVglYakUs+z72nvU5SVpL0hY1aSNgzUlJW8gjBU6mxQ6XUPzltWVATkZ9wPH+WGlJKgUsFrRZVMIltTkhXg2Ox+2HlwsXXlGCkLxnBTCYYbovojt0CD87fTcKZUd9aN5LLjd+RWEkI9HbRndpqqEexuj4AmdnCxU1hml5Y5aAqA7OT7YScrqTgAlfc8uWZTdZSwstaGHSv5/ZBSkztdV0Sy0t4Fu2RROhY/dgJUjmVfA4DUWCDlxv0lNQYozK38ONZ2Zc/2OPtrb/zo7Mf7IJkZw00lGG6I6rfU7HycLR6ofOZmKk7HpSEps/y7JDuq5Ahws0dgEzsEPLDYKXmX4koV5pUKQ8WBJ+uufgDSPU+qfJyRTKEfQPQCSjnLg68p7Op+RkUI7U0mU24AKTH6oSflBpB+G1UGMXvPckJP8WMHL45nMjKGm0ow3BA1LEIIxKfl4kxcKk7fTMX5W+mITsrC7bQcVPbbzcNRWRx0SoUfNzv4ONvevwMzVV9Bzv1usCINYON0P6TUh4lVC/OA1Lji0BN9P/SUhKG89Mr3lym0N4F09gfUzbSDu+Wq4kWp/9W6gvVltlcxMJXCcFMJhhuixiG3QIOY5GxEJ2XielIWou9mITpJu5R3pVYJmZUEH2cbXfAJcLPThR9PRxWsLO1SdTI+IbQ3eSx9pqf0GaC0OMOODSpNpqhmEHogOCnstbcR0C2ODzwvXurRAGuGm0ow3BBRWnYBopOzEJ2Uiei7WdrwU7xUNgeXytoK/q52CHSzux9+mmjDj7OdhV/BRcajKQQybt8PPem3tVd5FeZpx/mUfC3I1X9emKedDuTB7YwVlMojtykn9FQQhMpdX7zOBDesZLipBMMNEVVECIHEjDxc153lyUR0kjb8xCZno7C8Wy8Xc7K11o3naeZkAy8nG3iqVfBW28DLSQVHVf35HzKZmabwgRBUeikVhArKCUaFudr1JbcZyMvQdqnpHhcvFc6xVktW1vqBx9ELGP2TQQ/BcFMJhhsiqo0CTRFupeTowk5J8Im+m4XbaVVchQPtDQo9iycbLQk8pWdd91LbcJBzLeQWaBCdlIWoOxm4mpiJqDsZyM7XoKOvM7oFuqKjr1PDvk1AbWkKygaeioJQXnoF6zMqHkju2AyYft6gJTPcVILhhogMLSdfgxvJ97u2bqfmID4tV/c1LaegWu04qOSlgo+NbuZ1bycbXRCyUTTOP9S5BRpcv5uFK4kZuHInUxdmbiRnlTuXWQmF3AodfZzwSKArHmHYMbwiTfGNJh8IQwJASH+DHorhphIMN0Rkatn5hdqJRkvNsF4y6Wh8mjYAZeRWb5yFk6219myPWqXt9ioVfLyK19XnP965BRpcu5uJK3cycSUxA1F3MnE1MRMxlYQYR5UczT0cEOJhjxB3ByjkVjh24x4OXUtGYob+bQIYduovhptKMNwQkSXKzCtEfKp2dvWEB4KPNhjlIKuSwc6lOarkcLFT6BZn21KP7RRwsdV+dS1+7qiSm/xmh7kFGlxNzCx1JiYTVxMzEHsvu8IQo7axRnMPewS7O6B5cZBp7mEPNwdlufULIRCdlIXD1+/h8PVkHL5eftjp5Hs/7HTwYdixVAw3lWC4IaL6SAiB9NzC+4En9f7Zn4T0HN1ZodyCohq3LbeS4GRbEnasdYGoJPw8GJBc7BTVDgA5+cVnYorPwly5k4EriZmIvZdd4X2InGyt0dzdAcEe9mjubo+Q4rMybvblh5jqejDsHLqejLsMO/UGw00lGG6IqKESQiA1uwDJWfm4V7ykZJd6nJWPe9nar8nFz6t7NuhBNtayB84GWevOBmXmaXC1OMzEpVQcYpxtrbXBxd1e261UHGSa2Jtm2gyGnfqF4aYSDDdERPflFmiQml2gC0Al4aei5ynZ+SjQ1OzPhoudoji4aENMcHGYcbWwub+EELielFXchaUNPA+GHaXcCp18nYvDjgs6+DpBKWfYMQWGm0ow3BAR1Z4QApl5hUjJKsC97Hzcy8rDvawC3Vmhe5n5UMit9MbGuNorzV12rRgj7BQVCeQVFiGnQKNd8guRk1/6uQa5pR7nFBQ/L35c9nkRcku/lq9BXmERgtzt8XCACx4OcEHXABc0qaffg9IYbirBcENERLVR3bDT2tsRVpKkFzjuB5Oaj4kyhODisPNQgAseCXSFh2M9mO/rAQw3lWC4ISIiQ3gw7By6llzhDPXlUcqtYKOQwcZau6isZbrn9x9bwVYh1z63lsFGYVVmWxtrGVSlHltJEs7eSsXR6Hs4cv0eLt/JKHNsP1fb4rDjiocDXNDM2caiugjLw3BTCYYbIiIyBiEErt3NwsX4dFjL9IOLjcKqVECRQSWXmWwS1pSsfBy9cU8bdqKTceF2epnL7b3VKjwc6IqHiruyAprYWVzYYbipBMMNERE1Zum5BThxIwVHisPOuZtpZeZNc3NQ6oLOwwGuCHG3N1kYqwjDTSUYboiIiO7Lzi/EyZhUHI1OxuHoezgdl4r8Qv2xQc621ujq74KHA7XdWC29HCEzcdhhuKkEww0REVHFcgs0OBOXiiPR2q6sEzEpyCnQvx+Sg1KOLv7Ouq6stk3VsJZZGbUuhptKMNwQERFVX35hEf6+nYYj1+/haHQyjt9IQUae/lxoNtYydPZz1l2R1d4INztkuKkEww0REVHtaYoELsan4/D1ZByNvoejN+4hNVt/5ntbhQwn5w0waMCpyd9vucGOSkRERA2ezEpCm6ZqtGmqxis9A1FUJHAlMRNHopO1g5Sv30NTJ/POTs9wQ0RERLVmZSUh1NMBoZ4OGNvNH0IIpOUUVL2jMWsy69GJiIioQZEk7Szz5sRwQ0RERA0Kww0RERE1KAw3RERE1KAw3BAREVGDwnBDREREDYpZw014eDi6du0KBwcHuLu7Y8SIEbh8+XKV+0VGRqJz585QqVQIDAzEqlWrTFAtERER1QdmDTeRkZEICwvD4cOHsWfPHhQWFmLgwIHIysqqcJ/o6GgMGTIEPXv2xKlTpzBnzhxMnjwZGzduNGHlREREZKksavqFu3fvwt3dHZGRkejVq1e528yaNQtbtmzBxYsXdesmTJiAM2fO4NChQ1Ueg9MvEBER1T81+fttUWNu0tLSAAAuLi4VbnPo0CEMHDhQb92gQYNw/PhxFBSUvSNiXl4e0tPT9RYiIiJquCwm3AghMH36dDz66KNo06ZNhdslJCTAw8NDb52HhwcKCwuRlJRUZvvw8HCo1Wrd4uPjY/DaiYiIyHJYTLiZOHEizp49ix9++KHKbSVJ0nte0rP24HoAmD17NtLS0nRLXFycYQomIiIii2QRE2dOmjQJW7ZswR9//IFmzZpVuq2npycSEhL01iUmJkIul8PV1bXM9kqlEkql0qD1EhERkeUy65kbIQQmTpyITZs2Yd++fQgICKhyn27dumHPnj1663bv3o0uXbrA2traWKUSERFRPWHWMzdhYWH4/vvv8euvv8LBwUF3RkatVsPGxgaAtlvp1q1b+OabbwBor4xasWIFpk+fjldffRWHDh3Cl19+Wa3uLOB+FxYHFhMREdUfJX+3q3WRtzAjAOUuX3/9tW6bcePGid69e+vtFxERITp27CgUCoXw9/cXK1eurPYx4+LiKjwuFy5cuHDhwsWyl7i4uCr/1lvUfW5MoaioCLdv34aDg0O5A5DrIj09HT4+PoiLi2uU99Bp7O8f4GfQ2N8/wM+A779xv3/AeJ+BEAIZGRnw9vaGlVXlo2osYkCxKVlZWVU5aLmuHB0dG+0PNcD3D/AzaOzvH+BnwPffuN8/YJzPQK1WV2s7i7kUnIiIiMgQGG6IiIioQWG4MSClUon58+c32vvqNPb3D/AzaOzvH+BnwPffuN8/YBmfQaMbUExEREQNG8/cEBERUYPCcENEREQNCsMNERERNSgMN0RERNSgMNwYyOeff46AgACoVCp07twZBw4cMHdJJhMeHo6uXbvCwcEB7u7uGDFiBC5fvmzusswmPDwckiRh6tSp5i7FpG7duoV//OMfcHV1ha2tLTp06IATJ06YuyyTKCwsxNtvv42AgADY2NggMDAQixYtQlFRkblLM5o//vgDw4YNg7e3NyRJwubNm/VeF0JgwYIF8Pb2ho2NDfr06YPz58+bp1gjqOz9FxQUYNasWWjbti3s7Ozg7e2NsWPH4vbt2+Yr2Aiq+hko7V//+hckScKyZctMUhvDjQFs2LABU6dOxdy5c3Hq1Cn07NkTgwcPRmxsrLlLM4nIyEiEhYXh8OHD2LNnDwoLCzFw4EBkZWWZuzSTO3bsGFavXo127dqZuxSTSklJQY8ePWBtbY2dO3fiwoUL+PDDD+Hk5GTu0kzi/fffx6pVq7BixQpcvHgRS5YswdKlS/Hpp5+auzSjycrKQvv27bFixYpyX1+yZAk++ugjrFixAseOHYOnpycGDBiAjIwME1dqHJW9/+zsbJw8eRLz5s3DyZMnsWnTJkRFReHJJ580Q6XGU9XPQInNmzfjyJEj8Pb2NlFlgFknzmwoHnroITFhwgS9dS1atBBvvfWWmSoyr8TERAFAREZGmrsUk8rIyBAhISFiz549onfv3mLKlCnmLslkZs2aJR599FFzl2E2Q4cOFS+//LLeuqeeekr84x//MFNFpgVA/PLLL7rnRUVFwtPTU7z33nu6dbm5uUKtVotVq1aZoULjevD9l+fo0aMCgIiJiTFNUSZW0Wdw8+ZN0bRpU/H3338LPz8/8fHHH5ukHp65qaP8/HycOHECAwcO1Fs/cOBAHDx40ExVmVdaWhoAwMXFxcyVmFZYWBiGDh2K/v37m7sUk9uyZQu6dOmCZ599Fu7u7ujYsSPWrFlj7rJM5tFHH8Xvv/+OqKgoAMCZM2fw559/YsiQIWauzDyio6ORkJCg93tRqVSid+/ejfr3oiRJjeZsJqCdqHrMmDGYOXMmWrdubdJjN7qJMw0tKSkJGo0GHh4eeus9PDyQkJBgpqrMRwiB6dOn49FHH0WbNm3MXY7JrF+/HidOnMDx48fNXYpZXL9+HStXrsT06dMxZ84cHD16FJMnT4ZSqcTYsWPNXZ7RzZo1C2lpaWjRogVkMhk0Gg3effddjBo1ytylmUXJ777yfi/GxMSYoySzys3NxVtvvYUXX3yxUU2m+f7770Mul2Py5MkmPzbDjYFIkqT3XAhRZl1jMHHiRJw9exZ//vmnuUsxmbi4OEyZMgW7d++GSqUydzlmUVRUhC5dumDx4sUAgI4dO+L8+fNYuXJlowg3GzZswLfffovvv/8erVu3xunTpzF16lR4e3tj3Lhx5i7PbPh7UTu4+IUXXkBRURE+//xzc5djMidOnMDy5ctx8uRJs3zP2S1VR02aNIFMJitzliYxMbHM/1oaukmTJmHLli3Yv38/mjVrZu5yTObEiRNITExE586dIZfLIZfLERkZiU8++QRyuRwajcbcJRqdl5cXWrVqpbeuZcuWjWZQ/cyZM/HWW2/hhRdeQNu2bTFmzBhMmzYN4eHh5i7NLDw9PQGg0f9eLCgowHPPPYfo6Gjs2bOnUZ21OXDgABITE+Hr66v7vRgTE4M33ngD/v7+Rj8+w00dKRQKdO7cGXv27NFbv2fPHnTv3t1MVZmWEAITJ07Epk2bsG/fPgQEBJi7JJPq168fzp07h9OnT+uWLl26YPTo0Th9+jRkMpm5SzS6Hj16lLn8PyoqCn5+fmaqyLSys7NhZaX/61QmkzXoS8ErExAQAE9PT73fi/n5+YiMjGw0vxdLgs2VK1ewd+9euLq6mrskkxozZgzOnj2r93vR29sbM2fOxK5du4x+fHZLGcD06dMxZswYdOnSBd26dcPq1asRGxuLCRMmmLs0kwgLC8P333+PX3/9FQ4ODrr/ranVatjY2Ji5OuNzcHAoM77Izs4Orq6ujWbc0bRp09C9e3csXrwYzz33HI4ePYrVq1dj9erV5i7NJIYNG4Z3330Xvr6+aN26NU6dOoWPPvoIL7/8srlLM5rMzExcvXpV9zw6OhqnT5+Gi4sLfH19MXXqVCxevBghISEICQnB4sWLYWtrixdffNGMVRtOZe/f29sbzzzzDE6ePIlt27ZBo9Hofi+6uLhAoVCYq2yDqupn4MFAZ21tDU9PT4SGhhq/OJNck9UIfPbZZ8LPz08oFArRqVOnRnUZNIByl6+//trcpZlNY7sUXAghtm7dKtq0aSOUSqVo0aKFWL16tblLMpn09HQxZcoU4evrK1QqlQgMDBRz584VeXl55i7NaPbv31/uv/tx48YJIbSXg8+fP194enoKpVIpevXqJc6dO2feog2osvcfHR1d4e/F/fv3m7t0g6nqZ+BBprwUXBJCCONHKCIiIiLT4JgbIiIialAYboiIiKhBYbghIiKiBoXhhoiIiBoUhhsiIiJqUBhuiIiIqEFhuCEiIqIGheGGiIyqT58+mDp1qrnL0CNJEjZv3mzuMojISHgTPyIyqnv37sHa2hoODg7w9/fH1KlTTRZ2FixYgM2bN+P06dN66xMSEuDs7AylUmmSOojItDi3FBEZlYuLi8HbzM/Pr9P8PCWzVhNRw8RuKSIyqpJuqT59+iAmJgbTpk2DJEmQJEm3zcGDB9GrVy/Y2NjAx8cHkydPRlZWlu51f39/vPPOOxg/fjzUajVeffVVAMCsWbPQvHlz2NraIjAwEPPmzUNBQQEAYO3atVi4cCHOnDmjO97atWsBlO2WOnfuHB577DHY2NjA1dUVr732GjIzM3Wvjx8/HiNGjMAHH3wALy8vuLq6IiwsTHcsIrIsDDdEZBKbNm1Cs2bNsGjRIsTHxyM+Ph6ANlgMGjQITz31FM6ePYsNGzbgzz//xMSJE/X2X7p0Kdq0aYMTJ05g3rx5ALQzsq9duxYXLlzA8uXLsWbNGnz88ccAgOeffx5vvPEGWrdurTve888/X6au7OxsPP7443B2dsaxY8fw008/Ye/evWWOv3//fly7dg379+/HunXrsHbtWl1YIiLLwm4pIjIJFxcXyGQyODg46HULLV26FC+++KJuHE5ISAg++eQT9O7dGytXroRKpQIAPPbYY5gxY4Zem2+//bbusb+/P9544w1s2LABb775JmxsbGBvbw+5XF5pN9R3332HnJwcfPPNN7CzswMArFixAsOGDcP7778PDw8PAICzszNWrFgBmUyGFi1aYOjQofj99991Z5GIyHIw3BCRWZ04cQJXr17Fd999p1snhEBRURGio6PRsmVLAECXLl3K7Pvzzz9j2bJluHr1KjIzM1FYWAhHR8caHf/ixYto3769LtgAQI8ePVBUVITLly/rwk3r1q0hk8l023h5eeHcuXM1OhYRmQbDDRGZVVFREf71r39h8uTJZV7z9fXVPS4dPgDg8OHDeOGFF7Bw4UIMGjQIarUa69evx4cfflij4wsh9Mb/lFZ6vbW1dZnXioqKanQsIjINhhsiMhmFQgGNRqO3rlOnTjh//jyCg4Nr1NZff/0FPz8/zJ07V7cuJiamyuM9qFWrVli3bh2ysrJ0Aeqvv/6ClZUVmjdvXqOaiMgycEAxEZmMv78//vjjD9y6dQtJSUkAtFc8HTp0CGFhYTh9+jSuXLmCLVu2YNKkSZW2FRwcjNjYWKxfvx7Xrl3DJ598gl9++aXM8aKjo3H69GkkJSUhLy+vTDujR4+GSqXCuHHj8Pfff2P//v2YNGkSxowZo+uSIqL6heGGiExm0aJFuHHjBoKCguDm5gYAaNeuHSIjI3HlyhX07NkTHTt2xLx58+Dl5VVpW8OHD8e0adMwceJEdOjQAQcPHtRdRVXi6aefxuOPP46+ffvCzc0NP/zwQ5l2bG1tsWvXLty7dw9du3bFM888g379+mHFihWGe+NEZFK8QzERERE1KDxzQ0RERA0Kww0RERE1KAw3RERE1KAw3BAREVGDwnBDREREDQrDDRERETUoDDdERETUoDDcEBERUYPCcENEREQNCsMNERERNSgMN0RERNSgMNwQERFRg/L/E1hEFLlOE0AAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the training and test error\n",
        "plt.figure()\n",
        "plt.plot(log_epoch, log_train, label='train loss')\n",
        "plt.plot(log_epoch, log_test, label='test loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.title('training and test loss over time')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0A_Mkx_qptf"
      },
      "source": [
        "Our Neural Network that we have made using python ends up with a train and test loss of around 2.2 and 2.3 respectively. Compared to our PyTorch model this is considerably worse as we were unable to introduce the number of layers and complexity within our model as we did using PyTorch due to the slow computation time it took within python. With each epoch taking around 2-3 minutes to run compared to pytorchs 20-30 second epochs with a more complex Neural Network. The advantage about PyTorch is that it uses a hybrid stucture, using C++ for all the heavy computations such as convolutions and automatic differentiation and python to allow the writing of the training loops and model definitions. Since C++ computes a lot faster than Python this is the main reason as to why our Neural Network is considerably slower than the PyTorch model in question 1. That being said, our model is robustly running off all of our previously defined classes producing a correct Neural Network for the CIFAR-10 data."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Owa3-PIsQq8t",
        "_2GDuMnqwI7P",
        "no1y9tRBrzZ8",
        "kNiN4iTJMcOU",
        "P4zILHOXuN8x",
        "RYZnID4dvCUB",
        "vwFCw4X-cJr5",
        "u_tUEQZLcakZ",
        "iTvwX6NM0b5e",
        "q-uIpFiX2LJr",
        "IlJWOUONWYRJ",
        "bTwTMHm2Whik",
        "fGyx_ZW-WnWc",
        "Uksaa48sW2QD",
        "Vo2B8STG33tv",
        "uM8TM4uv4Stq",
        "892hEMtI-U6G",
        "Fmdkik0N7YCG",
        "P9-3UxDZ_AgB",
        "c2zgc6kL_MEH",
        "UceTMwGa_hKz",
        "lx9UzbWL_nz0",
        "bhwSbODs_n7I",
        "fPyzFsdWAeyx",
        "yC9VAjZFAnFv",
        "uV_blcdJKsmJ",
        "ROJ1slud_n_S",
        "yeKwGn-IAUYm",
        "bKaqht6oPYLT",
        "B7Y25aFhjDLv",
        "QflbiyijcMeU",
        "trBwxokiNdL6",
        "oaSJsh6GNdbh",
        "V7Zr2JJvNdmO",
        "pR_7jTDiNdvz",
        "wOcNA-uNPlhO",
        "3SxL3lH1YROQ",
        "m04gCZUOGt8Q",
        "fIUkr_l1kSMg",
        "asOMoVcXkNyk",
        "8LvLsvIUKJWH",
        "K7rRlDuZdL1P",
        "Ke-BlMiCfCVJ",
        "RUUB8o6Di9dO",
        "A5rBKYmWMXdI",
        "-Oi91vf9hOD6",
        "zblw7Y27hfEi",
        "eSKkocIKqIi8",
        "g9IDgP38nOqu"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
